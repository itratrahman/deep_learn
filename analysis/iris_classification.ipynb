{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS DATA CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains neural network to classify iris data. The training manifesto given in the notebook follows the caviar search strategy for hyperparameter tuning, a technique is followed by machine learning scientists and engineers for training deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import deep_learn package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from deep_learn.nn import ann\n",
    "except:\n",
    "    from config import *\n",
    "    append_path('../')\n",
    "    from deep_learn.nn import ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import neccessary package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and reshape data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load iris data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store the data in pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the X y data horizontally\n",
    "data = np.hstack((X,y))\n",
    "# store the numpy array in pandas dataframe\n",
    "data = pd.DataFrame(data=data, columns=iris.feature_names +['species'])\n",
    "# shuffle the data\n",
    "data = data.sample(frac=1, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                 5.8               4.0                1.2               0.2   \n",
       "1                 5.1               2.5                3.0               1.1   \n",
       "2                 6.6               3.0                4.4               1.4   \n",
       "3                 5.4               3.9                1.3               0.4   \n",
       "4                 7.9               3.8                6.4               2.0   \n",
       "5                 6.3               3.3                4.7               1.6   \n",
       "6                 6.9               3.1                5.1               2.3   \n",
       "7                 5.1               3.8                1.9               0.4   \n",
       "8                 4.7               3.2                1.6               0.2   \n",
       "9                 6.9               3.2                5.7               2.3   \n",
       "10                5.6               2.7                4.2               1.3   \n",
       "11                5.4               3.9                1.7               0.4   \n",
       "12                7.1               3.0                5.9               2.1   \n",
       "13                6.4               3.2                4.5               1.5   \n",
       "14                6.0               2.9                4.5               1.5   \n",
       "15                4.4               3.2                1.3               0.2   \n",
       "16                5.8               2.6                4.0               1.2   \n",
       "17                5.6               3.0                4.5               1.5   \n",
       "18                5.4               3.4                1.5               0.4   \n",
       "19                5.0               3.2                1.2               0.2   \n",
       "\n",
       "    species  \n",
       "0       0.0  \n",
       "1       1.0  \n",
       "2       1.0  \n",
       "3       0.0  \n",
       "4       2.0  \n",
       "5       1.0  \n",
       "6       2.0  \n",
       "7       0.0  \n",
       "8       0.0  \n",
       "9       2.0  \n",
       "10      1.0  \n",
       "11      0.0  \n",
       "12      2.0  \n",
       "13      1.0  \n",
       "14      1.0  \n",
       "15      0.0  \n",
       "16      1.0  \n",
       "17      1.0  \n",
       "18      0.0  \n",
       "19      0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features and output of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = iris.feature_names\n",
    "output = 'species'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data for deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:` I did not use a separate validation set in this excercise since the iris data only contains 150 examples. It is difficult to split such a small data into train, validation, and test sets. According to literature it is ok to leave out the validation set in such case. For such a small data K fold cross validation is a viable option; but K fold cross validation is used for normal machine learning algorithms, not deep learning algorithms. Deep learning algorithms works best in the big data regime, training deep learning models takes more time than training normal machine learning models. Hence, engineers do not use K fold cross validation for training deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do a train test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size = 0.266, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A function to extract feature matrix and output vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy_data(dataframe, features = None, output = None):\n",
    "\n",
    "    '''a function for parsing the feature matrix and output array from a pandas dataframe'''\n",
    "\n",
    "    # to ignore pandas warning\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # import numpy\n",
    "    import numpy as np\n",
    "\n",
    "    # if no featues are given then just return the a numpy matrix of the dataframe\n",
    "    if features == None:\n",
    "        return dataframe.as_matrix()\n",
    "\n",
    "    # extract the feature matrix and convert it to numpy array\n",
    "    X = dataframe[features].as_matrix()\n",
    "\n",
    "    # if there is no output\n",
    "    if output == None:\n",
    "        return X\n",
    "    # if the output vector is wanted by the user\n",
    "    else:\n",
    "        # extracting the output columns and converting it to numpy array\n",
    "        y = dataframe[output].as_matrix()\n",
    "        y = np.reshape(y, (-1,1))\n",
    "        # returning the feature matrix and output vector\n",
    "        return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract X y data for train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_xy_data(train_data, features=features, output=output)\n",
    "X_test, Y_test = get_xy_data(test_data, features=features, output=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Onehot encoding the y data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "Y_train = Y_train.toarray()\n",
    "Y_test = encoder.transform(Y_test)\n",
    "Y_test = Y_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110, 4)\n",
      "(110, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 4)\n",
      "(40, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the first neural network for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of first neural network is a dirty implementation which allows engineers to test if the network along with its hyperparameters, architecture, loss function actually works. After creating dirty implementation engineers do hyperparameter tuning. Now the iris dataset for is a very simple data to create a very accurate first implementation. Things will not so easy for example creating a yolo object detection network for detecting vehicles, pedestrians, and road signs for self driving system.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural network architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [4,4,8,8,4,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a nn model object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ann(layers_dims=layers_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = X_train.shape[0]\n",
    "learning_rate = 0.1*.5\n",
    "num_iterations = 40000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.141538\n",
      "Cost after iteration 100: 1.097231\n",
      "Cost after iteration 200: 1.101953\n",
      "Cost after iteration 300: 1.097959\n",
      "Cost after iteration 400: 1.095068\n",
      "Cost after iteration 500: 1.095910\n",
      "Cost after iteration 600: 1.096005\n",
      "Cost after iteration 700: 1.098199\n",
      "Cost after iteration 800: 1.102550\n",
      "Cost after iteration 900: 1.099853\n",
      "Cost after iteration 1000: 1.095382\n",
      "Cost after iteration 1100: 1.099318\n",
      "Cost after iteration 1200: 1.088701\n",
      "Cost after iteration 1300: 1.095299\n",
      "Cost after iteration 1400: 1.096558\n",
      "Cost after iteration 1500: 1.096727\n",
      "Cost after iteration 1600: 1.098543\n",
      "Cost after iteration 1700: 1.099408\n",
      "Cost after iteration 1800: 1.101593\n",
      "Cost after iteration 1900: 1.097817\n",
      "Cost after iteration 2000: 1.098948\n",
      "Cost after iteration 2100: 1.099111\n",
      "Cost after iteration 2200: 1.097732\n",
      "Cost after iteration 2300: 1.097172\n",
      "Cost after iteration 2400: 1.094945\n",
      "Cost after iteration 2500: 1.096410\n",
      "Cost after iteration 2600: 1.095079\n",
      "Cost after iteration 2700: 1.092685\n",
      "Cost after iteration 2800: 1.093785\n",
      "Cost after iteration 2900: 1.096380\n",
      "Cost after iteration 3000: 1.085926\n",
      "Cost after iteration 3100: 1.086663\n",
      "Cost after iteration 3200: 1.086084\n",
      "Cost after iteration 3300: 1.086688\n",
      "Cost after iteration 3400: 1.079885\n",
      "Cost after iteration 3500: 1.071297\n",
      "Cost after iteration 3600: 1.065327\n",
      "Cost after iteration 3700: 1.057986\n",
      "Cost after iteration 3800: 1.013874\n",
      "Cost after iteration 3900: 0.995266\n",
      "Cost after iteration 4000: 0.924267\n",
      "Cost after iteration 4100: 0.836937\n",
      "Cost after iteration 4200: 0.784408\n",
      "Cost after iteration 4300: 0.627154\n",
      "Cost after iteration 4400: 0.585327\n",
      "Cost after iteration 4500: 0.529606\n",
      "Cost after iteration 4600: 0.517681\n",
      "Cost after iteration 4700: 0.486039\n",
      "Cost after iteration 4800: 0.548227\n",
      "Cost after iteration 4900: 0.521728\n",
      "Cost after iteration 5000: 0.451106\n",
      "Cost after iteration 5100: 0.521100\n",
      "Cost after iteration 5200: 0.466852\n",
      "Cost after iteration 5300: 0.475298\n",
      "Cost after iteration 5400: 0.484668\n",
      "Cost after iteration 5500: 0.457975\n",
      "Cost after iteration 5600: 0.468992\n",
      "Cost after iteration 5700: 0.491565\n",
      "Cost after iteration 5800: 0.457090\n",
      "Cost after iteration 5900: 0.494358\n",
      "Cost after iteration 6000: 0.540699\n",
      "Cost after iteration 6100: 0.425706\n",
      "Cost after iteration 6200: 0.473048\n",
      "Cost after iteration 6300: 0.444992\n",
      "Cost after iteration 6400: 0.450718\n",
      "Cost after iteration 6500: 0.409078\n",
      "Cost after iteration 6600: 0.507683\n",
      "Cost after iteration 6700: 0.514407\n",
      "Cost after iteration 6800: 0.475052\n",
      "Cost after iteration 6900: 0.447033\n",
      "Cost after iteration 7000: 0.427223\n",
      "Cost after iteration 7100: 0.414007\n",
      "Cost after iteration 7200: 0.449520\n",
      "Cost after iteration 7300: 0.458441\n",
      "Cost after iteration 7400: 0.410357\n",
      "Cost after iteration 7500: 0.465695\n",
      "Cost after iteration 7600: 0.403759\n",
      "Cost after iteration 7700: 0.410157\n",
      "Cost after iteration 7800: 0.432819\n",
      "Cost after iteration 7900: 0.349529\n",
      "Cost after iteration 8000: 0.372084\n",
      "Cost after iteration 8100: 0.329165\n",
      "Cost after iteration 8200: 0.302499\n",
      "Cost after iteration 8300: 0.296917\n",
      "Cost after iteration 8400: 0.284296\n",
      "Cost after iteration 8500: 0.235990\n",
      "Cost after iteration 8600: 0.217064\n",
      "Cost after iteration 8700: 0.200890\n",
      "Cost after iteration 8800: 0.233887\n",
      "Cost after iteration 8900: 0.213202\n",
      "Cost after iteration 9000: 0.175328\n",
      "Cost after iteration 9100: 0.246614\n",
      "Cost after iteration 9200: 0.148774\n",
      "Cost after iteration 9300: 0.147712\n",
      "Cost after iteration 9400: 0.143669\n",
      "Cost after iteration 9500: 0.125833\n",
      "Cost after iteration 9600: 0.136769\n",
      "Cost after iteration 9700: 0.161923\n",
      "Cost after iteration 9800: 0.120981\n",
      "Cost after iteration 9900: 0.123116\n",
      "Cost after iteration 10000: 0.144125\n",
      "Cost after iteration 10100: 0.127515\n",
      "Cost after iteration 10200: 0.092186\n",
      "Cost after iteration 10300: 0.076727\n",
      "Cost after iteration 10400: 0.107118\n",
      "Cost after iteration 10500: 0.077702\n",
      "Cost after iteration 10600: 0.071347\n",
      "Cost after iteration 10700: 0.072004\n",
      "Cost after iteration 10800: 0.076023\n",
      "Cost after iteration 10900: 0.134649\n",
      "Cost after iteration 11000: 0.077602\n",
      "Cost after iteration 11100: 0.060490\n",
      "Cost after iteration 11200: 0.089926\n",
      "Cost after iteration 11300: 0.080683\n",
      "Cost after iteration 11400: 0.093115\n",
      "Cost after iteration 11500: 0.094916\n",
      "Cost after iteration 11600: 0.063480\n",
      "Cost after iteration 11700: 0.080398\n",
      "Cost after iteration 11800: 0.049399\n",
      "Cost after iteration 11900: 0.052444\n",
      "Cost after iteration 12000: 0.068840\n",
      "Cost after iteration 12100: 0.110633\n",
      "Cost after iteration 12200: 0.117500\n",
      "Cost after iteration 12300: 0.171030\n",
      "Cost after iteration 12400: 0.104685\n",
      "Cost after iteration 12500: 0.044344\n",
      "Cost after iteration 12600: 0.076577\n",
      "Cost after iteration 12700: 0.094206\n",
      "Cost after iteration 12800: 0.084340\n",
      "Cost after iteration 12900: 0.090057\n",
      "Cost after iteration 13000: 0.066107\n",
      "Cost after iteration 13100: 0.094670\n",
      "Cost after iteration 13200: 0.094415\n",
      "Cost after iteration 13300: 0.063620\n",
      "Cost after iteration 13400: 0.051361\n",
      "Cost after iteration 13500: 0.097158\n",
      "Cost after iteration 13600: 0.069109\n",
      "Cost after iteration 13700: 0.041060\n",
      "Cost after iteration 13800: 0.085837\n",
      "Cost after iteration 13900: 0.047142\n",
      "Cost after iteration 14000: 0.086959\n",
      "Cost after iteration 14100: 0.091103\n",
      "Cost after iteration 14200: 0.075235\n",
      "Cost after iteration 14300: 0.039425\n",
      "Cost after iteration 14400: 0.041641\n",
      "Cost after iteration 14500: 0.087002\n",
      "Cost after iteration 14600: 0.056866\n",
      "Cost after iteration 14700: 0.067571\n",
      "Cost after iteration 14800: 0.040358\n",
      "Cost after iteration 14900: 0.045372\n",
      "Cost after iteration 15000: 0.065317\n",
      "Cost after iteration 15100: 0.052113\n",
      "Cost after iteration 15200: 0.062728\n",
      "Cost after iteration 15300: 0.052232\n",
      "Cost after iteration 15400: 0.045335\n",
      "Cost after iteration 15500: 0.037355\n",
      "Cost after iteration 15600: 0.059230\n",
      "Cost after iteration 15700: 0.144452\n",
      "Cost after iteration 15800: 0.068376\n",
      "Cost after iteration 15900: 0.049710\n",
      "Cost after iteration 16000: 0.057913\n",
      "Cost after iteration 16100: 0.060318\n",
      "Cost after iteration 16200: 0.050659\n",
      "Cost after iteration 16300: 0.124472\n",
      "Cost after iteration 16400: 0.057547\n",
      "Cost after iteration 16500: 0.088775\n",
      "Cost after iteration 16600: 0.036774\n",
      "Cost after iteration 16700: 0.089323\n",
      "Cost after iteration 16800: 0.033823\n",
      "Cost after iteration 16900: 0.093966\n",
      "Cost after iteration 17000: 0.183675\n",
      "Cost after iteration 17100: 0.059407\n",
      "Cost after iteration 17200: 0.046297\n",
      "Cost after iteration 17300: 0.025516\n",
      "Cost after iteration 17400: 0.029251\n",
      "Cost after iteration 17500: 0.060717\n",
      "Cost after iteration 17600: 0.091489\n",
      "Cost after iteration 17700: 0.075355\n",
      "Cost after iteration 17800: 0.031489\n",
      "Cost after iteration 17900: 0.051996\n",
      "Cost after iteration 18000: 0.167617\n",
      "Cost after iteration 18100: 0.043271\n",
      "Cost after iteration 18200: 0.048977\n",
      "Cost after iteration 18300: 0.044606\n",
      "Cost after iteration 18400: 0.057873\n",
      "Cost after iteration 18500: 0.079249\n",
      "Cost after iteration 18600: 0.031190\n",
      "Cost after iteration 18700: 0.027122\n",
      "Cost after iteration 18800: 0.070908\n",
      "Cost after iteration 18900: 0.104234\n",
      "Cost after iteration 19000: 0.016092\n",
      "Cost after iteration 19100: 0.085568\n",
      "Cost after iteration 19200: 0.026616\n",
      "Cost after iteration 19300: 0.052972\n",
      "Cost after iteration 19400: 0.018711\n",
      "Cost after iteration 19500: 0.068132\n",
      "Cost after iteration 19600: 0.019239\n",
      "Cost after iteration 19700: 0.057989\n",
      "Cost after iteration 19800: 0.027523\n",
      "Cost after iteration 19900: 0.026035\n",
      "Cost after iteration 20000: 0.083647\n",
      "Cost after iteration 20100: 0.116517\n",
      "Cost after iteration 20200: 0.050174\n",
      "Cost after iteration 20300: 0.027252\n",
      "Cost after iteration 20400: 0.056130\n",
      "Cost after iteration 20500: 0.085318\n",
      "Cost after iteration 20600: 0.017564\n",
      "Cost after iteration 20700: 0.032119\n",
      "Cost after iteration 20800: 0.050925\n",
      "Cost after iteration 20900: 0.021125\n",
      "Cost after iteration 21000: 0.034156\n",
      "Cost after iteration 21100: 0.032036\n",
      "Cost after iteration 21200: 0.107726\n",
      "Cost after iteration 21300: 0.089142\n",
      "Cost after iteration 21400: 0.018240\n",
      "Cost after iteration 21500: 0.024275\n",
      "Cost after iteration 21600: 0.020017\n",
      "Cost after iteration 21700: 0.015403\n",
      "Cost after iteration 21800: 0.105771\n",
      "Cost after iteration 21900: 0.060250\n",
      "Cost after iteration 22000: 0.100183\n",
      "Cost after iteration 22100: 0.169862\n",
      "Cost after iteration 22200: 0.052679\n",
      "Cost after iteration 22300: 0.051355\n",
      "Cost after iteration 22400: 0.046148\n",
      "Cost after iteration 22500: 0.109101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 22600: 0.027459\n",
      "Cost after iteration 22700: 0.083282\n",
      "Cost after iteration 22800: 0.050851\n",
      "Cost after iteration 22900: 0.044547\n",
      "Cost after iteration 23000: 0.057898\n",
      "Cost after iteration 23100: 0.077795\n",
      "Cost after iteration 23200: 0.013936\n",
      "Cost after iteration 23300: 0.055414\n",
      "Cost after iteration 23400: 0.124773\n",
      "Cost after iteration 23500: 0.066637\n",
      "Cost after iteration 23600: 0.056938\n",
      "Cost after iteration 23700: 0.050754\n",
      "Cost after iteration 23800: 0.018727\n",
      "Cost after iteration 23900: 0.084728\n",
      "Cost after iteration 24000: 0.052523\n",
      "Cost after iteration 24100: 0.115389\n",
      "Cost after iteration 24200: 0.116157\n",
      "Cost after iteration 24300: 0.083357\n",
      "Cost after iteration 24400: 0.016763\n",
      "Cost after iteration 24500: 0.092948\n",
      "Cost after iteration 24600: 0.044386\n",
      "Cost after iteration 24700: 0.112470\n",
      "Cost after iteration 24800: 0.137813\n",
      "Cost after iteration 24900: 0.075653\n",
      "Cost after iteration 25000: 0.051496\n",
      "Cost after iteration 25100: 0.079519\n",
      "Cost after iteration 25200: 0.021277\n",
      "Cost after iteration 25300: 0.075983\n",
      "Cost after iteration 25400: 0.083984\n",
      "Cost after iteration 25500: 0.104605\n",
      "Cost after iteration 25600: 0.019768\n",
      "Cost after iteration 25700: 0.081126\n",
      "Cost after iteration 25800: 0.017601\n",
      "Cost after iteration 25900: 0.090737\n",
      "Cost after iteration 26000: 0.017524\n",
      "Cost after iteration 26100: 0.044486\n",
      "Cost after iteration 26200: 0.043170\n",
      "Cost after iteration 26300: 0.019579\n",
      "Cost after iteration 26400: 0.011469\n",
      "Cost after iteration 26500: 0.049445\n",
      "Cost after iteration 26600: 0.053665\n",
      "Cost after iteration 26700: 0.011038\n",
      "Cost after iteration 26800: 0.060985\n",
      "Cost after iteration 26900: 0.048634\n",
      "Cost after iteration 27000: 0.048783\n",
      "Cost after iteration 27100: 0.049509\n",
      "Cost after iteration 27200: 0.017901\n",
      "Cost after iteration 27300: 0.047500\n",
      "Cost after iteration 27400: 0.034137\n",
      "Cost after iteration 27500: 0.107944\n",
      "Cost after iteration 27600: 0.046724\n",
      "Cost after iteration 27700: 0.083411\n",
      "Cost after iteration 27800: 0.019248\n",
      "Cost after iteration 27900: 0.051373\n",
      "Cost after iteration 28000: 0.051630\n",
      "Cost after iteration 28100: 0.154676\n",
      "Cost after iteration 28200: 0.014475\n",
      "Cost after iteration 28300: 0.011534\n",
      "Cost after iteration 28400: 0.079046\n",
      "Cost after iteration 28500: 0.024172\n",
      "Cost after iteration 28600: 0.089739\n",
      "Cost after iteration 28700: 0.050234\n",
      "Cost after iteration 28800: 0.017284\n",
      "Cost after iteration 28900: 0.020675\n",
      "Cost after iteration 29000: 0.083822\n",
      "Cost after iteration 29100: 0.015396\n",
      "Cost after iteration 29200: 0.014831\n",
      "Cost after iteration 29300: 0.019092\n",
      "Cost after iteration 29400: 0.013592\n",
      "Cost after iteration 29500: 0.015956\n",
      "Cost after iteration 29600: 0.011959\n",
      "Cost after iteration 29700: 0.048825\n",
      "Cost after iteration 29800: 0.079506\n",
      "Cost after iteration 29900: 0.078767\n",
      "Cost after iteration 30000: 0.018929\n",
      "Cost after iteration 30100: 0.046713\n",
      "Cost after iteration 30200: 0.049740\n",
      "Cost after iteration 30300: 0.016665\n",
      "Cost after iteration 30400: 0.015962\n",
      "Cost after iteration 30500: 0.079708\n",
      "Cost after iteration 30600: 0.011605\n",
      "Cost after iteration 30700: 0.045254\n",
      "Cost after iteration 30800: 0.043062\n",
      "Cost after iteration 30900: 0.075973\n",
      "Cost after iteration 31000: 0.083404\n",
      "Cost after iteration 31100: 0.046072\n",
      "Cost after iteration 31200: 0.075988\n",
      "Cost after iteration 31300: 0.081139\n",
      "Cost after iteration 31400: 0.047834\n",
      "Cost after iteration 31500: 0.056627\n",
      "Cost after iteration 31600: 0.046652\n",
      "Cost after iteration 31700: 0.011921\n",
      "Cost after iteration 31800: 0.042386\n",
      "Cost after iteration 31900: 0.017118\n",
      "Cost after iteration 32000: 0.044602\n",
      "Cost after iteration 32100: 0.049470\n",
      "Cost after iteration 32200: 0.074906\n",
      "Cost after iteration 32300: 0.012929\n",
      "Cost after iteration 32400: 0.146536\n",
      "Cost after iteration 32500: 0.078071\n",
      "Cost after iteration 32600: 0.048422\n",
      "Cost after iteration 32700: 0.076857\n",
      "Cost after iteration 32800: 0.061080\n",
      "Cost after iteration 32900: 0.078548\n",
      "Cost after iteration 33000: 0.046067\n",
      "Cost after iteration 33100: 0.109253\n",
      "Cost after iteration 33200: 0.052368\n",
      "Cost after iteration 33300: 0.109296\n",
      "Cost after iteration 33400: 0.015114\n",
      "Cost after iteration 33500: 0.046102\n",
      "Cost after iteration 33600: 0.010742\n",
      "Cost after iteration 33700: 0.010537\n",
      "Cost after iteration 33800: 0.047841\n",
      "Cost after iteration 33900: 0.080777\n",
      "Cost after iteration 34000: 0.014627\n",
      "Cost after iteration 34100: 0.110278\n",
      "Cost after iteration 34200: 0.046011\n",
      "Cost after iteration 34300: 0.078266\n",
      "Cost after iteration 34400: 0.108602\n",
      "Cost after iteration 34500: 0.107082\n",
      "Cost after iteration 34600: 0.043325\n",
      "Cost after iteration 34700: 0.009922\n",
      "Cost after iteration 34800: 0.113525\n",
      "Cost after iteration 34900: 0.010147\n",
      "Cost after iteration 35000: 0.009856\n",
      "Cost after iteration 35100: 0.052732\n",
      "Cost after iteration 35200: 0.014798\n",
      "Cost after iteration 35300: 0.110366\n",
      "Cost after iteration 35400: 0.044366\n",
      "Cost after iteration 35500: 0.011175\n",
      "Cost after iteration 35600: 0.042309\n",
      "Cost after iteration 35700: 0.012535\n",
      "Cost after iteration 35800: 0.010831\n",
      "Cost after iteration 35900: 0.014372\n",
      "Cost after iteration 36000: 0.047036\n",
      "Cost after iteration 36100: 0.043836\n",
      "Cost after iteration 36200: 0.056075\n",
      "Cost after iteration 36300: 0.045658\n",
      "Cost after iteration 36400: 0.045166\n",
      "Cost after iteration 36500: 0.075616\n",
      "Cost after iteration 36600: 0.011896\n",
      "Cost after iteration 36700: 0.042145\n",
      "Cost after iteration 36800: 0.041663\n",
      "Cost after iteration 36900: 0.012112\n",
      "Cost after iteration 37000: 0.109495\n",
      "Cost after iteration 37100: 0.044709\n",
      "Cost after iteration 37200: 0.014499\n",
      "Cost after iteration 37300: 0.010141\n",
      "Cost after iteration 37400: 0.050193\n",
      "Cost after iteration 37500: 0.081021\n",
      "Cost after iteration 37600: 0.044517\n",
      "Cost after iteration 37700: 0.072723\n",
      "Cost after iteration 37800: 0.010954\n",
      "Cost after iteration 37900: 0.021951\n",
      "Cost after iteration 38000: 0.046208\n",
      "Cost after iteration 38100: 0.042099\n",
      "Cost after iteration 38200: 0.078032\n",
      "Cost after iteration 38300: 0.013275\n",
      "Cost after iteration 38400: 0.047146\n",
      "Cost after iteration 38500: 0.012360\n",
      "Cost after iteration 38600: 0.044210\n",
      "Cost after iteration 38700: 0.044475\n",
      "Cost after iteration 38800: 0.044482\n",
      "Cost after iteration 38900: 0.115442\n",
      "Cost after iteration 39000: 0.010687\n",
      "Cost after iteration 39100: 0.077500\n",
      "Cost after iteration 39200: 0.016892\n",
      "Cost after iteration 39300: 0.047485\n",
      "Cost after iteration 39400: 0.011253\n",
      "Cost after iteration 39500: 0.016864\n",
      "Cost after iteration 39600: 0.136349\n",
      "Cost after iteration 39700: 0.141368\n",
      "Cost after iteration 39800: 0.045631\n",
      "Cost after iteration 39900: 0.009295\n",
      "Cost after iteration 40000: 0.108390\n",
      "Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, X_test, Y_test, batch_size,\n",
    "          learning_rate = learning_rate, \n",
    "          num_iterations = num_iterations, print_cost=True, random_seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot of Cost vs Iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd4HNXV+PHvkVar3i3Lttw7xhjbGNPBtAQTQoeYkIS0FwIh/FLeJBAIcSDkTSAkQCiBECBAQk+C6b0YF1xwwd3GVZaLrN7r/f0xZWdXu5JMtLuSdT7Po0e7s7MzZ0ere+aWuSPGGJRSSimAhHgHoJRSqvfQpKCUUsqlSUEppZRLk4JSSimXJgWllFIuTQpKKaVcmhRUvyAir4nIFfGOQ6neTpOCiioR2S4iZ8Q7DmPMbGPM3+MdB4CIvC8i343BfpJF5BERqRaRvSLy4y7W/5G9XpX9vmTPa9tFpEFEau2fN6Mdv4oPTQqqzxMRX7xjcPSmWIC5wDhgBHAq8DMROSvciiLyReB64HRgJDAa+HXIal82xmTYP1+IVtAqvjQpqLgRkXNEZKWIVIrIQhGZ4nntehH5TERqRGSdiFzgee2bIrJARP4kIuXAXHvZRyLyBxGpEJFtIjLb8x737Lwb644SkQ/tfb8tIveJyJMRPsMsESkWkZ+LyF7gURHJFZGXRaTU3v7LIjLUXv824CTgXvuM+157+UQReUtEykVko4hc2gOH+BvArcaYCmPMeuCvwDcjrHsF8DdjzFpjTAVwayfrqkOYJgUVFyIyHXgEuArIBx4E5nmaLD7DKjyzsc5YnxSRwZ5NHANsBQYCt3mWbQQGALcDfxMRiRBCZ+v+E1hixzUX+HoXH2cQkId1Rn4l1v/Vo/bz4UADcC+AMeZGYD5wrX3Gfa2IpANv2fsdCFwG3C8ih4fbmYjcbyfScD+r7XVygSHAKs9bVwFht2kvD123UETyPcv+YSe6N0XkyC6OieqjNCmoePkf4EFjzMfGmDa7vb8JOBbAGPOcMabEGNNujHkG2AzM9Ly/xBjzZ2NMqzGmwV62wxjzV2NMG/B3YDBQGGH/YdcVkeHA0cDNxphmY8xHwLwuPks78CtjTJMxpsEYU2aMecEYU2+MqcFKWqd08v5zgO3GmEftz/MJ8AJwcbiVjTHXGGNyIvw4ta0M+3eV561VQGaEGDLCrItn/cuxmpVGAO8Bb4hITiefSfVRmhRUvIwAfuI9ywWGYZ3dIiLf8DQtVQKTsc7qHbvCbHOv88AYU28/zAizXmfrDgHKPcsi7cur1BjT6DwRkTQReVBEdohINfAhkCMiiRHePwI4JuRYXI5VA/m8au3fWZ5lWUBNJ+uHrouzvjFmgZ3w6o0x/wdUYtXk1CFGk4KKl13AbSFnuWnGmKdEZARW+/e1QL4xJgdYA3ibgqI1ve8eIE9E0jzLhnXxntBYfgJMAI4xxmQBJ9vLJcL6u4APQo5FhjHm6nA7E5G/eEYBhf6sBbD7BfYA3maeI4G1ET7D2jDr7jPGlHXymSM1zak+TJOCioUkEUnx/PiwCv3vicgxYkkXkS+JSCaQjlXolAKIyLewagpRZ4zZASzD6rz2i8hxwJcPcjOZWP0IlSKSB/wq5PV9WKN7HC8D40Xk6yKSZP8cLSKHRYjxe55RQKE/3j6Dx4Gb7I7viVhNdo9FiPlx4DsiMsnuj7jJWVdEhovICfbxSBGRn2LV2hYcxDFRfYQmBRULr2IVks7PXGPMMqxC6l6gAtiCPdrFGLMOuBNYhFWAHkFsC6DLgeOAMuA3wDNY/R3ddReQChwAFgOvh7x+N3CxPTLpHrvf4QvAHKAEq2nr90Ay/51fYXXY7wA+AO4wxrwObkFfa/ehYC+/Hau/YIf94ySzTOABrL/TbuAsYHYntQjVh4neZEepzonIM8AGY0zoGb9ShxytKSgVwm66GSMiCWJd7HUe8J94x6VULEQtKYh1mfx+EVkT4fXLRWS1/bNQxz2rXmQQ8D7WiJx7gKuNMSviGpFSMRK15iMRORnrn+pxY0yHTkIROR5Yb4ypsK8mnWuMOSYqwSillOqWqM3TYoz5UERGdvL6Qs/TxcDQaMWilFKqe3rL5F3fAV6L9KKIXIk1fQDp6elHTZw4MVZxKaXUIWH58uUHjDEFXa0X96QgIqdiJYUTI61jjHkIeAhgxowZZtmyZTGKTimlDg0isqM768U1KYg1K+bD6JhnpZTqFeI2JNW+aOZfwNeNMZviFYdSSqmAqNUUROQpYBYwQESKsa6OTAIwxvwFuBlrauL77RmLW40xM6IVj1JKqa5Fc/TRZV28/l0g6rckVEop1X16RbNSSimXJgWllFIuTQpKKaVc/SYpbNhbze2vb6CyvjneoSilVK/Vb5LCjrJ67n//M4orGrpeWSml+ql+kxQGZlr3K9lf09jFmkop1X/1n6SQlQLAvuqDuYGWUkr1L/0mKRRk2DUFTQpKKRVRv0kKfl8Ceel+bT5SSqlO9JukAFa/wv4arSkopVQk/SopFGQm89a6fby9bp+7bFd5PT97fhXLd1TQ2NKG9050za3tvLl2Ly+u3E1dUytVDS3xCFsppWIm7vdTiIfvPr6MS44aSkaKj5dX76G0polnlxUD8PVjR5CV6mPj3ho+2nKAxpb2oPeOG5jB1GE5pCf7OH5MPoOzU3l97R6unjWWjOR+eTiVUoeQflWKXXXyGEbkp1Hf3MZzy4tJSUrgiKJs7r98One8vpEl28t5YrF1H4qxAzM4f2oRsyYMZHdlAzWNLSSI8MnOCv61YjcJAo8t3O5uOzs1iStPHhOnT6aUUj1DvM0lfUFP3XmtrqmVNH8i9rTdGGP4eFs5cx5azPWzJ/K9UyIX8LVNrST7EnhtzV6qG1p4fNF2Nu2rpSgnFYA/f3Ua04fn/tcxKqVUTxGR5d25PUG/TQqR7CqvZ2huqpssumP5jnL+vnAHIvDiyhLGDczgtf93Er7EftVlo5TqxbqbFPpV81F3DMtLO+j3HDUij6NG5AFw9hGDueqJ5fxrxW4unTGsp8NTSqmo0lPZHvaFSYVMGZrNH97YSFmtDn9VSvUtmhR6mIjwfxceQWV9C3NfWhfvcJRS6qBoUoiCw4dkc/WsMby0qoTVxZXxDkcppbpNk0KUXHq01Z+wrqQ6zpEopVT3aVKIEncCPp1WQynVh2hSiBK/L4HctCSdgE8p1adoUoiiwqwUvX+DUqpP0aQQRQU6K6tSqo/RpBBFAzNTKK3W5iOlVN+hSSGKBmZZNYX29r41lYhSqv+KWlIQkUdEZL+IrInwuojIPSKyRURWi8j0aMUSL4OzU2htN+zTzmalVB8RzZrCY8BZnbw+Gxhn/1wJPBDFWOLiiKJsAFbs1AvYlFJ9Q9SSgjHmQ6C8k1XOAx43lsVAjogMjlY88XD4kGxSkhJYur2zw6CUUr1HPPsUioBdnufF9rIORORKEVkmIstKS0tjElxP8PsSmDosh092VMQ7FKWU6pZ4JoVwNywI2yNrjHnIGDPDGDOjoKAgymH1rGG5aXqtglKqz4hnUigGvDccGAqUxCmWqMlKTaKmsSXeYSilVLfEMynMA75hj0I6FqgyxuyJYzxRkZnio665jda29niHopRSXYranddE5ClgFjBARIqBXwFJAMaYvwCvAmcDW4B64FvRiiWeslKSAKhpbCU33R/naJRSqnNRSwrGmMu6eN0A34/W/nuLrFRNCkqpvkOvaI6yrBQr71Zrv4JSqg/QpBBlmXbzUXWDJgWlVO+nSSHKslKdmkJrnCNRSqmuaVKIMqejWZuPlFJ9gSaFKPN2NCulVG+nSSHKMpLt5iPtU1BK9QGaFKIsMUHITPZp85FSqk/QpBADmSk+bT5SSvUJmhRiIC3ZR32zJgWlVO+nSSEG0pN91Da1xTsMpZTqkiaFGMhITqSuSWsKSqneT5NCDKT7fZoUlFJ9giaFGMhI9lGrSUEp1QdoUoiBtORE6pu1T0Ep1ftpUoiBdK0pKKX6CE0KMZDh99Hc2k6L3n1NKdXLaVKIgXR7qgvtbFZK9XaaFGLAmf9Im5CUUr2dJoUYSEtOBNDOZqVUr6dJIQbStaaglOojNCnEQIb2KSil+ghNCjGQ7tekoJTqGzQpxEBmit6nWSnVN2hSiIHcdD8AFXXNcY5EKaU6p0khBtL9ifh9CZRrUlBK9XKaFGJARBiQ7qdMk4JSqpeLalIQkbNEZKOIbBGR68O8PlxE3hORFSKyWkTOjmY88ZSX4aestineYSilVKeilhREJBG4D5gNTAIuE5FJIavdBDxrjJkGzAHuj1Y88ZaXnqzNR0qpXi+aNYWZwBZjzFZjTDPwNHBeyDoGyLIfZwMlUYwnrvK1+Ugp1QdEMykUAbs8z4vtZV5zga+JSDHwKvCDcBsSkStFZJmILCstLY1GrFGXn+7XmoJSqteLZlKQMMtMyPPLgMeMMUOBs4EnRKRDTMaYh4wxM4wxMwoKCqIQavTlZfipb26jQec/Ukr1YtFMCsXAMM/zoXRsHvoO8CyAMWYRkAIMiGJMcZOXZl2rUF6vtQWlVO8VzaSwFBgnIqNExI/VkTwvZJ2dwOkAInIYVlLom+1DXUj1WzOlNrZoTUEp1XtFLSkYY1qBa4E3gPVYo4zWisgtInKuvdpPgP8RkVXAU8A3jTGhTUyHBH+idaibW/Xua0qp3ssXzY0bY17F6kD2LrvZ83gdcEI0Y+gt/D5NCkqp3k+vaI4RNynofZqVUr2YJoUY0eYjpVRfoEkhRrT5SCnVF2hSiJFknzX6qEmTglKqF9OkECPap6CU6gs0KcRIsp0UmvQ6BaVUL6ZJIUa0pqCU6gs0KcSIjj5SSvUFmhRiREcfKaX6Ak0KMaJJQSnVF2hSiBFfgiCifQpKqd5Nk0KMiAj+xAStKSilejVNCjHk9yXoxWtKqV5Nk0IMJfsStPlIKdWraVKIIW0+Ukr1dpoUYkibj5RSvZ0mhRjy+xJobtVpLpRSvZcmhRiykoLWFJRSvZcmhRjyJ2pHs1Kqd9OkEEPJvkStKSilejVNCjGkzUdKqd5Ok0IM6egjpVRvp0khhvx68ZpSqpfTpBBDyYkJNLVoUlBK9V6aFGIoM8VHdWNLvMNQSqmIupUUROSS7ixTnSvMTqGmsZX65tZ4h6KUUmF1t6ZwQzeXBRGRs0Rko4hsEZHrI6xzqYisE5G1IvLPbsbTJxVmpgCwr7opzpEopVR4vs5eFJHZwNlAkYjc43kpC+j0dFdEEoH7gDOBYmCpiMwzxqzzrDMOK7mcYIypEJGBn+9j9A2Dsq2ksLeqkVED0uMcjVJKddRpUgBKgGXAucByz/Ia4EddvHcmsMUYsxVARJ4GzgPWedb5H+A+Y0wFgDFmf/dD73sKs5yaQmOcI1FKqfA6TQrGmFXAKhH5pzGmBUBEcoFhTkHeiSJgl+d5MXBMyDrj7W0uABKBucaY10M3JCJXAlcCDB8+vIvd9l5OTUGTglKqt+pun8JbIpIlInnAKuBREfljF++RMMtMyHMfMA6YBVwGPCwiOR3eZMxDxpgZxpgZBQUF3Qy598lI9pGR7GOvJgWlVC/V3aSQbYypBi4EHjXGHAWc0cV7ioFhnudDsZqjQtd50RjTYozZBmzEShKHrIGZyeyv0Y5mpVTv1N2k4BORwcClwMvdfM9SYJyIjBIRPzAHmBeyzn+AUwFEZABWc9LWbm6/T8pL91NR1xzvMJRSKqzuJoVbgDeAz4wxS0VkNLC5szcYY1qBa+33rQeeNcasFZFbRORce7U3gDIRWQe8B/zUGFP2eT5IX5Gb7qdck4JSqpfqavQRAMaY54DnPM+3Ahd1432vAq+GLLvZ89gAP7Z/+oW8ND+riyvjHYZSSoXV3Suah4rIv0Vkv4jsE5EXRGRotIM7FOWm+6moa8HKh0op1bt0t/noUaz+gCFYQ01fspepg5SXnkRzWzt1zXqvZqVU79PdpFBgjHnUGNNq/zwG9N2xoXGUm+YHoKKumZ8+t4rfvro+zhEppVRAd5PCARH5mogk2j9fAw7pDuFoyUu3kkJ5XTPPLS/moQ87H2y1ZFs5b6zdG4vQlFKq20nh21jDUfcCe4CLgW9FK6hDWa6TFOoDI5Da2iP3L1z64CKuemJ5xNeVUqondTcp3ApcYYwpMMYMxEoSc6MW1SEsz24+OuC5gG1XeX28wlFKqSDdTQpTvHMdGWPKgWnRCenQNig7hXR/Iu+sD8z9t35PNVUNevMdpVT8dTcpJNgT4QFgz4HUrWscVLCUpERmHzGY1z39BFf/4xOO/PWbHYapdtaspENalVLR0N2kcCewUERuFZFbgIXA7dEL69D2xcMHhV0eOky1rC7QxORNAs8t28XUW97SK6OVUj2uW0nBGPM41hXM+4BS4EJjzBPRDOxQFukGO+W1wYV8qaffoaXNSgqNLW389PnVVDW0sGV/bfSCVEr1S91uArLvmLauyxVVl4blpYZdXlbXxPD8NPe5Nyk0tLTh9yWwtyow7faeqoboBamU6pe623ykelCyL9F9PNi+8Q5ARX3kmkJTi9W0VO9pYvImCKWU6gnaWRxnw3LT2GMX7mV285Exhgm/fJ1ECdynqMFNCoFbY+/RpKCU6mFaU4iT7NQkAHLTk9xla3ZX8fW/fcwnOytobm13EwF4k0JgmTYfKaV6mtYU4uTNH51McUU9Ty8J3Mb6yY930tZuSPMndli/oTk4KRRkJmvzkVKqx2lNIU4Ks1I4akQeWamBmoJzXcIba/e5y7JSrLz97ceWsqOszm0+GluQwe5KTQpKqZ6lSSHOslICScGfmMDAzOSg14fmWqORKupbuOk/a9yawmGDszhQ20R1Y+BK6Ja2dp5asrPTi96UUqozmhTiLCvVqglMHZbDz2dPpDDLGo2Uk2Yli5SkwJ+osr7FbUaaMjQbgK2ldYA16+o972zmhn99yrPLAk1SSil1MDQpxFmmXVO47vSxfOfEUUwanAXA92eNBWDUgAx33erGFurs5qPJRdZ6n+2vpbWtnem3vsWf390CoPMoKaU+N+1ojrMcu08hO9WaPfXmL0/iiuNHMnFQJvkZfmaOyuOFT4oB2FFWz11vbybZl8CI/HR8CcJnpbW88umeoG0mCEop9bloUoizk8YP4NbzJzN1WA4A6ck+Jg2xagEXTh8a1GfgSE/2kZSYwMgB6WzaV9NhDqRW7VNQSn1O2nwUZ8m+RL5+7AgSI5zepyYFhqemhwxVnTI0m5W7KimuCL5eQZuPlFKflyaFXi4pMfAnuvX8yQDU2LWHacNzOVDbzOKtwXdGraxrYe68tXy8Ve+YqpQ6OJoU+pBB9sgkZ8bU6cOtJqfWdsPQ3MAke5v21/DYwu1c9aTexlMpdXC0T6EPGF+YwezJgyn0TJ4HMKEw0308dViO24y0YmclAAMygq95UEqprmhS6APe/NEpANQ2tQYt9yUmMHFQJhv21jBxUCYvrw4ehTQ8Lw2llDoYUW0+EpGzRGSjiGwRkes7We9iETEiMiOa8fR1Gckdc/jN50zClyCcOSn83dyUUupgRK2mICKJwH3AmUAxsFRE5tk36/GulwlcB3wcrVgOJRdOL+KoEe7tsjl+7AC2/PZsAG6YPZH/rCxh/Z5qAGobW8NuQymlIolm89FMYIsxZiuAiDwNnEfHu7fdinW/5/+NYiyHjD9eOjXia1edMoYpQ3N4dtkuDtQ2ufdnUEqp7opm81ER4J2Ep9he5hKRacAwY8zLnW1IRK4UkWUisqy0tLTnIz2EHDcmnz99ZSp56X53SgyllOquaCaFcFdjuZfaikgC8CfgJ11tyBjzkDFmhjFmRkFBQQ+GeOjKSPZp85FS6qBFMykUA8M8z4cCJZ7nmcBk4H0R2Q4cC8zTzuaekZHs6zBaSSmluhLNpLAUGCcio0TED8wB5jkvGmOqjDEDjDEjjTEjgcXAucaYZVGMqd9IT/bR1NreYV4kpZTqTNSSgjGmFbgWeANYDzxrjFkrIreIyLnR2q+yOMNXp9/6Fo2eez0rpVRnonrxmjHmVeDVkGU3R1h3VjRj6W+81zTsq25kRH56HKNRSvUVOvfRISojxZsUmuIYiVKqL9GkcIjyDv3aV90YtziUUn2LJoVDVL5nMjxNCkqp7tKkcIiaOSqPd35yCsm+BPbXaPORUqp7NCkcwsYUZFCYlaI1BaVUt2lSOMQVZiWzt0qTglKqezQpHOKKclLZVV4f7zCUUn2EJoVD3LjCTEqqGnXKC6VUt2hSOMSNHZgBwGf7a+MciVKqL9CkcIgbZyeFzZoUlFLdoEnhEDc8Lw2/L4GPNut9KJRSXdOkcIjzJSbw7RNG8Z+VJczXxKCU6oImhX7gx2eOJyctiReWF8c7FKVUL6dJoR/w+xKYPXkQb67bp9NoK6U6pUmhn5gxIo/65ja9kE0p1SlNCv3EgExrgrwDtToPklIqMk0K/cSADD+gSUEp1TlNCv1EgT2Vdmmt3rNZKRWZJoV+Ii/djwgc0Gm0lVKd0KTQT/gSE8hN83doPtpZVk9rW3ucolJK9TaaFPqRARnBSWFvVSMn3/Eet7+xMY5RKaV6E00K/ciAjGRKPc1H+2us4akLthyIV0hKqV5Gk0I/Mjg7lZ3l9by5di8jr3+F3RUNAPgSJM6RKaV6C00K/cjRI3M5UNvMb15ZD8A7G/YDkKhJQSll06TQjxwzOh+Anfad2JZtLwesTmillIIoJwUROUtENorIFhG5PszrPxaRdSKyWkTeEZER0YynvxuZn0ZhVrL7fHuZlRy0+Ugp5YhaUhCRROA+YDYwCbhMRCaFrLYCmGGMmQI8D9werXgUiAjH2rUFL20+Uko5ollTmAlsMcZsNcY0A08D53lXMMa8Z4xx7iq/GBgaxXgUcMwoKylo7UApFU40k0IRsMvzvNheFsl3gNeiGI8CThw7AF+CcNrEge6y+madTlspZfFFcdvhTkVN2BVFvgbMAE6J8PqVwJUAw4cP76n4+qXh+Wl89PPT2HqgljfX7QM0KSilAqJZUygGhnmeDwVKQlcSkTOAG4FzjTFhJ+YxxjxkjJlhjJlRUFAQlWD7k0HZKRTlpLrP65tb4xhNbBVX1NOgSVCpiKKZFJYC40RklIj4gTnAPO8KIjINeBArIeyPYiwqxKDsFPdxf6kpGGM48ffvceUTy+IdilK9VtSSgjGmFbgWeANYDzxrjFkrIreIyLn2ancAGcBzIrJSROZF2JzqYcm+RPdxfzlzbmq1Jv6bv1mn9VAqkqhep2CMedUYM94YM8YYc5u97GZjzDz78RnGmEJjzFT759zOt6h60u0XTWHa8BzqmlsxJmx3z3/t7Lvn88U/fRiVbR+srmpEDc1tvPrpnhhFE6yqvoWz7vqQzftq4rJ/pRx6KWs/dunRwzhzUiHGBM6i91Y18t7GnmvJW7enmo29pKCra+q872TuvLVc849PWLWrMkYRBbyzYR8b9tZw33tbYr5vpbw0KfRzaUlWM5JTYF5w/wK+9ehSWg7BeyzU2R3qEuESDWf6j9oukkc0NNtJ2e/rW/+Sza3tlNfp3fwOJX3rG6h6XFqyNSq5rslqWtlTZU2nXdoDd2iLVpPU5+UkvkgX7hl7xHQ8LutzknBSH5uH6v89vYLpt74V7zBUD+pb30DV43JSkwA4976P2Hagzl2+p6qRBVsO/FcFe43njLupNbqd2Z8WV7G7sqHTdZzElxChqhDPHNbUR2sKr63ZCxDXu/c9PH8rzyzdGbf9H2r61jdQ9bjTJg7krq9MBeCaf3ziLn/ko21c/vDHPLVkV6S3dqm8NtCsUNXQ8vmD7IYv3/sRJ/zu3U7X6aqm4GiOQwHn7NPfgzWFmsYW9lU39tj2OhOPY+b4zSvr+fkLn8Zt/9G0ZFs55937ESVdnPD0JE0K/ZwvMYHzpxVx3pFDWL+n2l3+waZSAJbtKO/wnl3l9cydt5a29s5Prcs8bc3VUU4K3VFnjz5KiNh8ZGlsiX0BV9vY8/0YX/jThxzz23d6fLvhNMXhmB3q2tsNlz64iFXFVTEdrKFJQQEwc1Tw7KlOZ2u4kThvrdvHYwu3ux2zkVR4kkJlffSSQnebuLqsKdibiXZTVzjVjdbxaWzpuX07/UOx4DR/qZ7T6PkeNsbwWiJNCgqAmaPyAPji4YVk2/0MAJ+V1lHTGFygV9ZbhX1Xo068r0ez+ShcgbRmdxUTbnqNPVWBarcz+qirqcJ7smDurqoGK7aGLvb9aXEVI69/ha2ltZ2u5x09FouRZPFIpIc670WldZoUVKwVZCbzynUncvecaRw90koQXzvWmnwwtAO3wj7rd5JDJKW1gRFM0awphBtC+uTiHTS1tvPehlJ3WVfXKTijjz5v81FlfTNPLt7xuTrnnea1rvb97xW7AXhnffC1JMt3lPPYgm1uQttRFhg0EO3+HOgdNYX2Lpoze7O2dsPS7cFNtY2eY9oQw/nJNCko1+FDsklJSuTer05j/S1ncdF06/YWxeWhSaHrmkJtUytPLNrhTrxXGcWCKVxh7wwwMp6JeZ3RR5EKXqcs/7w1hR89s5Kb/rOGzfs7P4sPx2k+cmoKO8vqwxZyvkTrg7WFJJ6LHljE3JfW8ZY98+2mfYEYopmQHb2hT6HWU3A2trSxo6yOXeX1Yb8fy3dUsKOsjpa2dvfk5pmlOznvvgXuOutKqlmzu+qgYiipbGB/zcE32939zmYu+csiVuysCPoMjrrmNrYfqKMqBn9LTQqqg5SkRFL9iQzNTQNg5a5KTvvD++4UDJVuTSHyF3TJtjL2Vjdy2wWTEYnu2aq3puCcpYudFbzlqjMbbKRCv7OO5oq6ZpaH6XT3cgriSAVkW7th/I2v8dcPt7rLtpbWctfbm9zj09jSxtbSWk6+4z3uf7/j1c3OcFpvJ7/38zj9PN5RR1UNgeTd3NrOf1bs/ny1mcYWRl7/Cv/8uOPwz3g1H3mHwnoHM/z8hdWccsf7nHT7e3zz0SUd3nfRAws55Y73uevtTUy95S3ufXcz60qqWbWr0k3GZ98zn3P+/NFBxXP8795l5m0H37nvJJ8yz4g979+1vqmVL971IffHI3NHAAAgAElEQVR/EP0r3jUpqIgGZPhJ9iXw8Edb2XqgjgftwsytKXTSfOR0ck4YlElWShJVnazb3NreZdNOZ5waAATOtJ1egybv2Za9Xmu7CTuu3ml7bwxTwF38l4Vc9MCiTgtTp2CvbgyfAGsaW2hua+e2V9e7y656Yjl3vb2ZraVWc09jSxu7Kqya2eKtkZOQt7nGm3B32vfd9i7zJu/fv76BHz6zko+2HPykgPvsv+nD87d2eC1ezUfe/VY3BL5D3ua1pdsriOTT3daIu4+3lbvfnXh8Fue759QEITgp1DS10tTaTlpSNG+BY9GkoCISEYpyU90z53a7QKwM6VPYUVbHD59ewb9XFLvv3VfVSIJAQUYy2alJVDa08Pa6fTy3zLru4ZXVe/jmo0v4YFMpd765kQvuX+DdtTvtQ3fUNgUKvRp7aKdTdFd7hnrWeZsX7O23txt3lJTTsRfuTP8zt9COHJdTY4lUK6pp7FijCR3W29jS7iascB3iTvL09ud4C/2d5fUs31HBJs8QRm88C+xk8Hku1HMKy9CmK+u1+NQUgpKCJxlHmsok1E6776WyvsWdMDEe9xdpbbOOqffQer9rB+waRJo/kWiLftpRfdqIvDT3LNYpfJyaQnFFA+fdt4D1JdU0t7Wzs7yeC6ZZ/RD7qpsYkJGMLzGBnLQkqhpa+O7j1n0MfvfaBtqMobK+hWRfAi1thk37aqltaqWxpY11JdV845El3H7xFC6dMYzlO8oZW5BJdlpSmAih1lNTqGlspTArUHg++tE2CjL8zN98gF2eIbSNLW1kJPt4ZME2fvPKehZcf5pbQwhXU3AcqG2iKCe1w7UO3hqEUwi3trXj81yM5k0K2w7UMbogg7Tk4H/yhpY2t3BKSuxYsjkFX4UnETj7K8hMZmd5PRc9sBCAdH8idc1tQUlji93fUd/cxrYDdZx370e89IMTGZGfHvEzh+7bSWTez+wk0j+/s5ny+mZ+9eXDu9xeT/CeTXuTX2cjzLyJuNiulVU1tLgnBfXNbXgHaBtj3ObIaGltb3f37fB+tjJ70Ebo9yUatKagOnXcmMC/x7YDdTS1BgqtxVvLWLWr0r2ataSy0R31sre60b2RT3ZqUtAUGmV1zW5BtaOs3p1n6Q9vbGTGb952k8fzy4tpbm3nogcWcUWYdmGHt+nJGT7rFMA1Ta388sW1vLlun3u2D4F/uLfXWx2ziz4ro6G5Peg1h7fwO+n29/jVvLUdYvDOFVXV0MLCzw4wee4bHPCMwPIO7XWaiNL8gfOyzBQfjS1tbqf8/M0HuPfdzUH7cdrNnZrCU0t2cumDiwCYUpQdNAS3KDcVEauTv73dUNPYQqtdIK7fU80HG/dT3djKZ10Mbw3Ebx3T4ooGfv/6hqCrmJ0z9jvf2sSjC7YD8Pii7dz2yroO2zHGhO3X8dY2lm4v5yfPrqKt3TB/c2nEmkhw85GnpuBZx3tdSmtbO8t3BJqTWtsNaf5EKuub3e916LDgzzMc1PnOrC2p6jCkO5wWu6bgraU4cfgSxP0exaKmoElBdeqU8QPdxzvL69ldESh0nC8ywJemDGZvdSOn3PE+q3ZVsq+6kYGZgaSww27rvuK4EUHb315W53aKPrZwOxBoOlq1q5LiCut9KzuZzjo4KdjJIMI/YmaKVQg7VfN0u1BesOWAW1A5Z71z561l0s2vd+hQf2Lxjg7b3R+SFJZvr6Cxpd09E4XgDnEnvnTPP/noAek0trS5hVtTazt/eHMTxhje27Cf37y8zm0Oc2prf3hjo/v+CYMygzrWc9L8ZKcm8eTiHYz+xav8+qVAAX33O5uZaz+v6ebV1N71Hnj/s6Bx9GtLrOsnHMYYbn5xLX+dv63Ddu55ZwsTf/k6d74ZiH3x1jIm3PQ6y7aX09TaxiV/WcQLnxRzzT+W8/W/LQl7nwtjDJ96Rgct3V7Ow/O3csH9C4JqUsme+aReXFniJlHHlKHZ1DS1us2Loffd6Oxq/H3Vjawutr6b3ibP+uY2mlrb+NI9H/G9J5cHvecfH+/gj57PDoGawl/nb3U/q/MdzU33u81HqdqnoOJtfGEGZxw2kO+eOIq2dsNpd34AwEz7WgaA+T87lS9MKnSfbztgFfSDspMByLGbfRIEfvGlwxiYaS0/fkw+jS3tQQWq4+iRuTS1tvPuhq7v7eAtbJ3HkQq6QVlWonISgDNa55OdFe4y5/djC7dT39zGVk8tx6ut3bhnhKEX6jk1I2+B4o3J6RT15FVGDkinsaW9w/Uf+2ua+NZjS3n4o21uE0lFnfXb26Q2PC8t6H1ZKUnkp/vd2J5fXhz0d3Nj6XZSCC4cD3hGyrz66d6g1zobrrxil3Wm/oln+OWH9rQqiz4rY8XOwAnAG2utmtyu8o5z/zw8fxvXPbXCff7ssmJ+88r6oPdD8CSD4WpFRw7NwRjYX219D+ubW4MK+M5Gzp18+3uce+8C933e9zgxL9hSxnn3fuQ2W9347zXc827wKKKWVuu1Tftq3TnInO+h92+oNQUVdyLCw1cczU3nTOJnZ01g+vAc7vrKVK49bay7zrC8NEZ62qTX76mmor6FwdnWNQo5qX4ABmenkuxLJC/den7y+IKI+z1utNVs5f0HP+F371Lf3Mof39rE/ppGt4bgrSk48y1FTAp2k1ZTaxvt7YYddlLYXdHgNq002q85Io1VP/3O9/nbR9aZcEV98DxPn9lJoSooKQQeO+3z3tiH5KTS0NLWoWbi1LIgMAKovK6ZtnZDbpo/6P1eWak+xhdmBi0bPyijQ3t7d+elCj2mez3TaDhnug7vFBuh11s4n8dJbBAYGCBC2OlTdoVZ9ua6vR2WheNNCt6aG0BKUgLj7GO0166xNra0BdWCOksKTvOVMSaohlHV0BJ0AeGq4qpOtxPuqnM3KWQE/sbp2qegepNrZo3lX9ecwPnTipgxMjfotZEDAknhqSXWOPYZI6x1nGkzBthf7j9fNo2vHjOcLx0x2H1PaEE13X6vt9lod2UDD8/fxj3vbGbmbe9w4f0LaW831Da1MSDDT25aEu9t2M+vXlwTcRrtwdlOTaGdvdWNNLe2M3pAupsQnNe2ef6h15Z0TAoNzW1sL6tn5a5K1uyuckcwjchPs2oK9hlpdVAi6Nh85E0KaUmJtLWboH4IgI82B67KrmlqZXheGg0tbby1bi8pSYF/4ZyQjvgEESYOygpalpeeTGpScMESLoGu2lXJNx5ZEra/xrHXcy1Ea1twwe9NGN4p1Fvb2t0C3ltIOt02IhI2Aewoq+9QcHprmE4NMBzvWb/THOkYkZfuTh/vqG9uo74l+Ky/K3XNbUE1hcr6lqB+NGc73k5u77DomjBDsp2k4E382nykeq00v48bZk/kb1fMAKyCf8mNp3P0yFyqG1tJTUpk2nA7KdiFVZb9zzeuMJPfXnAEwzzNHVOGZgdtf/SADPLT/R0K95dXl7iPN+6rYeFnZeytamBARjIzRubx7ob9/H1RxzZ/h1NIbthbw3a74D86pEmlsaWNpdsC1wisLakm1POfFNvx7OGcP3/EJntUz/C8NLYdqHMTQFVDC61t7RhjqG1qJSlRyElLcpuPnLPLX54ziVS7aWD9nuAZMZ9ZFjx9+YXTixiel8af3tocdLFTVkpw4dbc2s7EwcE1hfx0f4dhsN7C3hjDE4u286NnV/LhplLmrSrxrBdccK3cFWj+qQhp8irxdHh7ayK7K60aWVaKL+g9znDnFnsU27C8QK3nuNH5LNlezuy75wfVOrwX6DlNZ5OLstyTD4e3ozj0+zQiP61DMq1vamPeysDnDk0Kxhi+8uAi5noGHFQ1tISpKQQnoIr65qCrncOdJHg1trSTmCBBc5Fp85Hq1a46ZQynHxboSxiYmeI2YcyaUOBW253RH9mpHYeUXjbTml/pxLEDgpYPzEqmKNfa1oAMP3/9hpV8vNM3ZKcmcc+7m/l0dzWHD8lm3MAMwOqrAPj6sSP482XTgrY7ZWg2Ewdl8uqne9x/2qNHBSeFtSXVXP+vwPz86/ZUB52RA/zyP2uCnq/aVUl2ahK5af6gJopd5Q2MvfE1Hl+0g5rGFjJTkshKSQpqPvrqMcP5zomjGGp/3r0h90DYVx1ccxiYmcLccyexcV8NG/ZaCaQgM7nD8W1pa2dCSPNRbrq/wxm3t7D/87tb+OWLa91hyHe+uYmz757PvFUlvLI6uLP3ycWBK5tDZ+TY7Pk7eWtL2+1jPnV4LvXNbdz84hq+/89P3L6gyvoWth+oC+ofGZxj1QK27K/l/U1WH9PeqsagcfwpdmF5zpQhXGhPz+Jobm1na2ktTa1tHY7lyAHpHY7b3xdt5/9e2xCI35MUGlvauPudzXy8rdwdGGHF3Rx0EWV1Q4t70uGoqm9h4ZYy9/kLy4tpbLE6pEOvf3lqyU7qmltJ8SWQnhyoHWhSUH3O/5w0mutOH8ftF09xlzlNQyPDjIX/zfmTeenaE93+hcxkH5OLskhJSmSI3SdRkJnCmZ6O7MuPGc7SG8/gZ2dNYMm2cg7UNnFEURbfPnEUPzxjHI9+62ge+eYM/vcLE5g4KLhQzEnzM3vyYJbvqGDVrkqSEoWpwwK1FG/hP+foYYDVtDFqQEann3ttSTW5aUmMzA8UZiKBDtR/fLyDmsZWMpJ9ZKX63IK4rtlaBnDW5MG8cPXxQdt14vfWpAZlJ3PaxEIKs6wO+68eM5zFN5zu1sQcuel+Rg5I540fnuwuy0/3BzWVAcxbVcLJt7/Hgi0H3PtoOEYPSGfdnmque2pF2CaObx4/kqyUjk0aazxNbt4rjZ2moSlF1ud5fNEOXlm9x02kjy3czqriKopyUimwByRcOmMYY+2E/8SiHdz77mZ++MyKoJFbPzhtLCePL+DSGcM4drSV5B+4fDo/P2siAKfd+UHYvqGJgzpe/7IlZO6qdSXVPLpgG8YYnl6yk7veDh4mDFbyvPe9wPKK+uYOU5cv3lbGT55b5T6/7dX1XHj/Qo669e0O27vhX5/y4soSUpIS3RFyELh9bjRpUlA9anJRNj8+czyZnqaMc6YMYe6XJwV1TjsSE4QjhmYzvjCTzGQfd1xyJC//4CQg0E8xfXgOEKgBXHPqWAoyk5lz9HAmDbaag44Yms2AjGR+eMZ4kn2JnDaxkOy0JPIzkoP2l5OWxNGjrGatl1aVMCwvjUHZgaaK+y+fzlUnj+bOS47khrMPc8/MikI6ccPJTfczaUig8B6Rl+Y2V6Qn+yiuaCAzxWfVFOxmpcaW9qCzv6NG5LLkxtM5aZxVc7ru9HH87sIjuPW8ye46hXb7+TB7bqq8ND+JCRLUL/Ozsybwi7MPA6yhqm6MacFNK46d5fW8sLyYPZ7mlWnDc3j2e8eRnx78nmU3neE+vmBaESkhfRTp/sSgAtjb/FJS2YAvQRhXGJxk14YU2BdMG8q7PzmFFb88k2NH5/P2j0/hmFF5vLexlD+8uYnFW8uZY9cywTrWj397Jnnpfk6bWMj8n53K7CMGBzUNhU4b8p/vn8D5U4vICzkmodNc/GvFbn790jpKqhrdfoI/XHIkL1x9PD86YzwA727YzwJPLaC4ooG9VY1847gR/PSLEwD4YGNwwgWrFpqR7OO6MP8b5XXNpCQluqP4gA79QdGgSUFFXWKC8M0TRnUoPLyyU5NY/ssz+eLhgRrBVSeP5u/fnsktdoH4wOVHsfymM9wCOjFBuP3iKZw/dQiTi7LDbjcnNYnZkwcF7eeIomxErLbmkfnp7pk6wGkTC7nh7MO46KihZKcmMcBOKs5ZeWdy0/wcPiTQsZvjKWxW7Kxk+Y4KEkTc5qN6uyPReyYIVvOQU8D7ExOYM3M4YwYGClGnU9UZlRKuWe6qk8cEfS5Hfkb4pADW2f0+T+dtfrr1mYd7aj+jB6QHJYkR+WkkhzStTS7KDrqGxdt8tLuygcE5Ke62Hd679H3vlDEcNyafzJQkcj37ciZodIzzHJPkkO+W01/lTbhLtgUnhYmDMklIEHyJCUS6ADrPs/8dZXXsqmjgsMFZXHzUUI4akcslM4Z2eM+ADD+f7q6itqmVopxUvnfKGMDqx0rzJ/Lmj04OWn/GyFx+/IUJYfefkpRAUU7gc3d1L5CeoNNcqF4j9Kb1uel+TvEMWw03zcXkomzumjOtw3JHQoLwwNeOoqrBaqtOSkwgKTGBtCRrCojD7I7Ya08dG1SgO04Ym8/OJfUdOiPDyUlLcvsFIPw/8PdPHcs76/exaV8tiz6zzizTwxTeTsGfm27t19tU4hRUzlDf0EI50r6dGMMZkZ/m9tcUZiWzr7qJrFSfvW/r97dPGMU1p44JmvIhJ81Psi+4QD6iKJuPPQVwdUhNYUh2aqfH09vJ7OU9toDbpATBF6h5eRPu4q1lJCaI29Hufc+AjOSw18tMKMxk0Vbr77TtQB07yuoYNzBQ8wqXkCcNyXabDQdlWwk+K8VHdWMrEwZldvjsI/LTOmzDkZKUyJCcyCOrokFrCqpfyE5N4shhOe7zG84+jMuPGc73T7Wq7f/7xQnM9gyRddx63mR+9eVJfPWYwJXY18waE7TOuIEZXDitiPOnFiEiPPato3n9hye5tzI9faJ1VfgNsydy1uRBbg3hqiesK13DjT3/5TmTuHvOVI4aYbWPewti57FTuBzM/a9DC3DHuUcOcR8fOdQ+TvbJvlOIHjcm3605BW+zY03Ba8PeGv6+cDstbe2UVDZSlJPqFqZp/sQOfRLDcsMXkqFJYUxB10nBe6LR1NruDkmG4GMa7nNBcNPb1lKrpuCtOYXr+HWaNAH3Wh2n1jZxUGaHRDIiL/K8U6lJiR2uP4m2qCYFETlLRDaKyBYRuT7M68ki8oz9+sciMjKa8Sjl+NqxI7jtgiOC5h4Kx5eYwLdOGBXUp/CzsyZy2sSBjMxPY0CGn7vnTOOPX5nqdpbPmjCQiYOyOP0wKxn88dKp/PjM8Vxx/EggcGHeYXbhEdp8BFbt4bypRZ3G5rx+6sSBna4H8MYPT+a+r04H4K/fmBF0jQjAxUcFmkFG2X05Tof0l+2E4R3JdNH0oXz7hFFAoED+6jHD2f67L7k1rslFWWSm+Hh+eTG/mreWcTe+xu7KBgqzUyjITCbNn8htF0x29+cYlhc+KQzKDj5j9jYtRZqwLnSOpUh9QwMywycFbxJ5cvEOmlvbg+ILt98JgwLJynl/id3pfOakwg6JubNCPzPF12mzazRErflIRBKB+4AzgWJgqYjMM8Z4Z8j6DlBhjBkrInOA3wNfiVZMSv03Xv7Bie5Nbh755tFdrn/3nGnUNbWSnZbEdaePc5d/7dgRXDJjKA3NbTy6YDszR3eceiKca2aNCWp6mDQki+2/+1LQOh//4vQOc/eAdcbrnPWeOamQMycVckttE2l+HzVNLQzMTOG40fks2lrGsaPzefDDre76l80cxpemDA46w73z0iPdx2cfMZhPdlYywi4sxxVm8sFPZzE0N42nl+5k2fYKqhpa+HhrGXXNbUwozCQlKZF1t5xlfY7B2czfXMrb6/exeGt5xOYSZ/TadaeN5ZIZ1siw7544ikcWdJxfyXHqxIFcPWsMK3ZWsHhrOcPz0rh61pgOI4zGFmS4TT5gDak+cmgO4+1jkCBWTSM7NYmTQoZPg3WdxPjCDN5ev5/jxwReH2j3RWWmWCPOnLnEDh+SRWlNE/trmtxmsPx0P2V1zbz8gxN5fNF2nl1W7PZHxJJ8njswdWvDIscBc40xX7Sf3wBgjPk/zzpv2OssEhEfsBcoMJ0ENWPGDLNs2bKoxKxUf9bS1s72A3WMK8xk6fZypg/P7XbH5s6yegZmJXd5VruvupGCjOQOU4+DdVZfVtfc6Uiv/TXW+w92KuttB+p4f+N+zjisMGxNpLGljX9+vJPxhZnM31LKV2cOd6cT/2RnBZOHZLN4axnjCjPcJiFHdWMLSQkJ+BKFptZ2MpJ9fFZay5rdVW5tble5dUX2aE+TlzGGptZ295jtr26kpKqRqcNyqG5soaSywb3Y8q11+2htaw/bxNldIrLcGDOjy/WimBQuBs4yxnzXfv514BhjzLWeddbY6xTbzz+z14l4WyhNCkopdfC6mxSi2acQLpWHZqDurIOIXCkiy0RkWWlpx7G+SimlekY0k0IxMMzzfChQEmkdu/koG+hwY1pjzEPGmBnGmBkFBZFn1lRKKfXfiWZSWAqME5FRIuIH5gDzQtaZB1xhP74YeLez/gSllFLRFbXRR8aYVhG5FngDSAQeMcasFZFbgGXGmHnA34AnRGQLVg1hTrTiUUop1bWoXtFsjHkVeDVk2c2ex43AJdGMQSmlVPfpFc1KKaVcmhSUUkq5NCkopZRyRe3itWgRkVIg8v0WOzcAiHhhXBz11rig98amcR0cjevgHIpxjTDGdDmmv88lhf+GiCzrzhV9sdZb44LeG5vGdXA0roPTn+PS5iOllFIuTQpKKaVc/S0pPBTvACLorXFB741N4zo4GtfB6bdx9as+BaWUUp3rbzUFpZRSndCkoJRSytVvkkJX94uOcSzbReRTEVkpIsvsZXki8paIbLZ/58YgjkdEZL99syNnWdg4xHKPffxWi8j0GMc1V0R228dspYic7XntBjuujSLyxSjGNUxE3hOR9SKyVkT+n708rsesk7jiesxEJEVElojIKjuuX9vLR9n3ZN9s36Pdby+PyT3bO4nrMRHZ5jleU+3lMfvu2/tLFJEVIvKy/Ty2x8sYc8j/YM3S+hkwGvADq4BJcYxnOzAgZNntwPX24+uB38cgjpOB6cCaruIAzgZew7ox0rHAxzGOay7wv2HWnWT/PZOBUfbfOTFKcQ0GptuPM4FN9v7jesw6iSuux8z+3Bn24yTgY/s4PAvMsZf/BbjafnwN8Bf78RzgmSgdr0hxPQZcHGb9mH337f39GPgn8LL9PKbHq7/UFGYCW4wxW40xzcDTwHlxjinUecDf7cd/B86P9g6NMR/S8aZGkeI4D3jcWBYDOSLy+W8Ye/BxRXIe8LQxpskYsw3YgvX3jkZce4wxn9iPa4D1QBFxPmadxBVJTI6Z/blr7adJ9o8BTgOet5eHHi/nOD4PnC5ykDdj/u/iiiRm330RGQp8CXjYfi7E+Hj1l6RQBOzyPC+m83+aaDPAmyKyXESutJcVGmP2gPVPDgyMU2yR4ugNx/Bau/r+iKd5LS5x2VX1aVhnmb3mmIXEBXE+ZnZTyEpgP/AWVq2k0hjTGmbfblz261VAfiziMsY4x+s2+3j9SUSSQ+MKE3NPuwv4GdBuP88nxservySFbt0LOoZOMMZMB2YD3xeRk+MYS3fF+xg+AIwBpgJ7gDvt5TGPS0QygBeAHxpjqjtbNcyyqMUWJq64HzNjTJsxZirW7XhnAod1su+4xSUik4EbgInA0UAe8PNYxiUi5wD7jTHLvYs72XdU4uovSaE794uOGWNMif17P/BvrH+WfU6V1P69P07hRYojrsfQGLPP/kduB/5KoLkjpnGJSBJWwfsPY8y/7MVxP2bh4uotx8yOpRJ4H6tNPkese7KH7rtb92yPUlxn2c1wxhjTBDxK7I/XCcC5IrIdq4n7NKyaQ0yPV39JCt25X3RMiEi6iGQ6j4EvAGsIvl/1FcCL8YivkzjmAd+wR2IcC1Q5TSaxENKGewHWMXPimmOPxBgFjAOWRCkGwbqF7HpjzB89L8X1mEWKK97HTEQKRCTHfpwKnIHV3/Ee1j3ZoePxivo92yPEtcGT2AWr3d57vKL+dzTG3GCMGWqMGYlVRr1rjLmcWB+vnuox7+0/WCMINmG1ad4YxzhGY438WAWsdWLBagt8B9hs/86LQSxPYTUrtGCddXwnUhxYVdX77OP3KTAjxnE9Ye93tf3PMNiz/o12XBuB2VGM60Ss6vlqYKX9c3a8j1knccX1mAFTgBX2/tcAN3v+B5ZgdXA/ByTby1Ps51vs10fHOK537eO1BniSwAilmH33PTHOIjD6KKbHS6e5UEop5eovzUdKKaW6QZOCUkoplyYFpZRSLk0KSimlXJoUlFJKuTQpqF5DRBbav0eKyFd7eNu/CLevaBGR80Xk5iht+xddr3XQ2zxCRB7r6e2qvkeHpKpeR0RmYc3uec5BvCfRGNPWyeu1xpiMnoivm/EsBM41xhz4L7fT4XNF67OIyNvAt40xO3t626rv0JqC6jVExJm58nfASfac9j+yJy+7Q0SW2pOVXWWvP0us+wj8E+uiIkTkP/ZEg2udyQZF5HdAqr29f3j3ZV+leoeIrBHrHhdf8Wz7fRF5XkQ2iMg/nBkoReR3IrLOjuUPYT7HeKDJSQhizdP/FxGZLyKb7DlunEnZuvW5PNsO91m+Jtb9AVaKyIMikuh8RhG5Taz7BiwWkUJ7+SX2510lIh96Nv8S1pW0qj+L9pV5+qM/3f0Bau3fs7Cv5rSfXwncZD9OBpZh3QdgFlAHjPKs61xNnIp1ZWq+d9th9nUR1uydiUAhsBPr/gSzsGadHIp18rQI68rhPKyrgJ1adk6Yz/Et4E7P88eA1+3tjMO6SjvlYD5XuNjtx4dhFeZJ9vP7gW/Yjw3wZfvx7Z59fQoUhcaPNffOS/H+HuhPfH+cSZaU6s2+AEwREWf+l2yswrUZWGKsewI4rhORC+zHw+z1yjrZ9onAU8ZqotknIh9gzZJZbW+7GECsaZZHAouBRuBhEXkFeDnMNgcDpSHLnjXWxHSbRWQr1mycB/O5IjkdOApYaldkUglMyNfsiW85cKb9eAHwmIg8C/wrsCn2A0O6sU91CNOkoPoCAX5gjHkjaKHV91AX8vwM4DhjTL2IvI91Rt7VtiNp8jxuA3zGmFYRmYlVGM8BrsWazdKrAauA9wrtvDN083N1QYC/G2NuCPNaizHG2W8b9v+7MeZ7InIM1s1cVorIVGNMGdaxapZ58fgAAAFySURBVOjmftUhSvsUVG9Ug3VbSccbwNViTQ+NiIwXa4bZUNlAhZ0QJmJN0+xocd4f4kPgK3b7fgHWrUAjzhgq1j0Lso0xrwI/xLpXQaj1wNiQZZeISIKIjMGa4GzjQXyuUN7P8g5wsYgMtLeRJyIjOnuziIwxxnxsjLkZOEBgWujxBGYGVf2U1hRUb7QaaBWRVVjt8XdjNd18Ynf2lhL+dqWvA98TkdVYhe5iz2sPAatF5BNjTUfs+DdwHNastQb4mTFmr51UwskEXhSRFKyz9B+FWedD4E4REc+Z+kbgA6x+i+8ZYxpF5OFufq5QQZ9FRG7CupNfAtbMst8HdnTy/jtEZJwd/zv2Zwc4FXilG/tXhzAdkqpUFIjI3Vidtm/b4/9fNsY838Xb4kasW09+AJxoArd+VP2QNh8pFR2/BdLiHcRBGA5crwlBaU1BKaWUS2sKSimlXJoUlFJKuTQpKKWUcmlSUEop5dKkoJRSyvX/ARvd1fQUEytiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the cost\n",
    "plt.plot(np.squeeze(model.costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Caviar Strategy for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 strategies for deep learning hyperparamter tuning: 1) Panda strategy in which we babysit a single model, this is applicable if computing resource is limited 2) Caviar strategy in which we randomly initialize a number of hyperparameter settings and train neural network model using different settings and then choose the one with the lowest error, this is strategy is applicable if we have enormous computing resource. In this notebook I will choose to tune the parameters of learning rate and nn architecture using caviar strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to generate a given number of nn architectures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer_and_node_generator(model_num, num_input, num_output, randome_seed = 0, low = 8, high=17):\n",
    "    \n",
    "    '''a function to generate a given number of nn architectures'''\n",
    "    \n",
    "    # set the random seed\n",
    "    np.random.seed(randome_seed)\n",
    "    \n",
    "    # list to store the architectures\n",
    "    model_architecture_list = []\n",
    "    \n",
    "    # iterate given number of times\n",
    "    for i in range(model_num):\n",
    "        # randomly generate number of hidden layers\n",
    "        num_hidden = np.random.randint(low = 3, high = 6)\n",
    "        # randomly generate the number of nodes in each layer\n",
    "        layers_dims = np.random.randint(low = low, high = high, size = num_hidden)\n",
    "        layers_dims = layers_dims.tolist()\n",
    "        # insert the input and output layer\n",
    "        layers_dims.insert(0,num_input)\n",
    "        layers_dims.append(num_output)\n",
    "        # append the architecture to the designated list\n",
    "        model_architecture_list.append(layers_dims)\n",
    "    \n",
    "    return model_architecture_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generaty a list of learning rates to choose from**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05  , 0.0523, 0.0545, 0.0568, 0.059 , 0.0613, 0.0636, 0.0658,\n",
       "       0.0681, 0.0704, 0.0726, 0.0749, 0.0771, 0.0794, 0.0817, 0.0839,\n",
       "       0.0862, 0.0884, 0.0907, 0.093 , 0.0952, 0.0975, 0.0997, 0.102 ,\n",
       "       0.1043, 0.1065, 0.1088, 0.1111, 0.1133, 0.1156, 0.1178, 0.1201,\n",
       "       0.1224, 0.1246, 0.1269, 0.1291, 0.1314, 0.1337, 0.1359, 0.1382,\n",
       "       0.1405, 0.1427, 0.145 , 0.1472, 0.1495, 0.1518, 0.154 , 0.1563,\n",
       "       0.1585, 0.1608, 0.1631, 0.1653, 0.1676, 0.1698, 0.1721, 0.1744,\n",
       "       0.1766, 0.1789, 0.1812, 0.1834, 0.1857, 0.1879, 0.1902, 0.1925,\n",
       "       0.1947, 0.197 , 0.1992, 0.2015, 0.2038, 0.206 , 0.2083, 0.2106,\n",
       "       0.2128, 0.2151, 0.2173, 0.2196, 0.2219, 0.2241, 0.2264, 0.2286,\n",
       "       0.2309, 0.2332, 0.2354, 0.2377, 0.2399, 0.2422, 0.2445, 0.2467,\n",
       "       0.249 , 0.2513, 0.2535, 0.2558, 0.258 , 0.2603, 0.2626, 0.2648,\n",
       "       0.2671, 0.2693, 0.2716, 0.2739, 0.2761, 0.2784, 0.2807, 0.2829,\n",
       "       0.2852, 0.2874, 0.2897, 0.292 , 0.2942, 0.2965, 0.2987, 0.301 ,\n",
       "       0.3033, 0.3055, 0.3078, 0.3101, 0.3123, 0.3146, 0.3168, 0.3191,\n",
       "       0.3214, 0.3236, 0.3259, 0.3281, 0.3304, 0.3327, 0.3349, 0.3372,\n",
       "       0.3394, 0.3417, 0.344 , 0.3462, 0.3485, 0.3508, 0.353 , 0.3553,\n",
       "       0.3575, 0.3598, 0.3621, 0.3643, 0.3666, 0.3688, 0.3711, 0.3734,\n",
       "       0.3756, 0.3779, 0.3802, 0.3824, 0.3847, 0.3869, 0.3892, 0.3915,\n",
       "       0.3937, 0.396 , 0.3982, 0.4005, 0.4028, 0.405 , 0.4073, 0.4095,\n",
       "       0.4118, 0.4141, 0.4163, 0.4186, 0.4209, 0.4231, 0.4254, 0.4276,\n",
       "       0.4299, 0.4322, 0.4344, 0.4367, 0.4389, 0.4412, 0.4435, 0.4457,\n",
       "       0.448 , 0.4503, 0.4525, 0.4548, 0.457 , 0.4593, 0.4616, 0.4638,\n",
       "       0.4661, 0.4683, 0.4706, 0.4729, 0.4751, 0.4774, 0.4796, 0.4819,\n",
       "       0.4842, 0.4864, 0.4887, 0.491 , 0.4932, 0.4955, 0.4977, 0.5   ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates = np.round(np.linspace(0.1*0.5,0.1*5,num=200),4)\n",
    "learning_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to randomly generate a given number of learning rates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_generator(learning_rates, model_num, randome_seed = 0):\n",
    "    \n",
    "    '''a function to randomly generate a given number of learning rates'''\n",
    "    \n",
    "    np.random.seed(randome_seed)\n",
    "    \n",
    "    return np.random.choice(learning_rates, size=model_num).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function which implements a caviar strategy search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caviar_strategy_search(model_num, batch_size, model_directory, randome_seed = 0, num_iterations = 40000):\n",
    "    \n",
    "    '''a function which implements a caviar strategy search'''\n",
    "    \n",
    "    # randomly generate a list of learning rates\n",
    "    learning_rate_list = learning_rate_generator(learning_rates, model_num, randome_seed = randome_seed)\n",
    "    # randomly generate a list of architectures \n",
    "    model_architecture_list = hidden_layer_and_node_generator(model_num,4,3, randome_seed = randome_seed)\n",
    "    # lists to store the costs and accuracy of models\n",
    "    cost_list = []\n",
    "    accuracy_list = []\n",
    "    \n",
    "    # iterate a given number of times\n",
    "    for i in range(model_num):\n",
    "        # create and fit a nn model with given architecture\n",
    "        model = ann(layers_dims=model_architecture_list[i])\n",
    "        model.fit(X_train, Y_train, X_test, Y_test, batch_size,\n",
    "                  learning_rate = learning_rate_list[i], \n",
    "                  num_iterations = num_iterations, print_cost=False, random_seed = randome_seed)\n",
    "        \n",
    "        #pickling the model\n",
    "        pickle_out = open(model_directory+\"iris_model_\"+str(i+1)+\".mdl\",\"wb\")\n",
    "        pickle.dump(model, pickle_out)\n",
    "        pickle_out.close()\n",
    "        \n",
    "        # append the average of last 5 costs to the designated list\n",
    "        cost_list.append(np.average(model.costs[-5:]))\n",
    "        # append the accuracy to the designated list\n",
    "        accuracy_list.append(model.accuracy)\n",
    "        \n",
    "        # print statement\n",
    "        print(\"Completed training and collected results for the model:\",str(i+1))\n",
    "        \n",
    "    return model_architecture_list, learning_rate_list, cost_list, accuracy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do a caviar strategy search of hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of models\n",
    "model_num = 20\n",
    "# batch size\n",
    "batch_size = X_train.shape[0]\n",
    "# random seed\n",
    "randome_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed training and collected results for the model: 1\n",
      "Completed training and collected results for the model: 2\n",
      "Completed training and collected results for the model: 3\n",
      "Completed training and collected results for the model: 4\n",
      "Completed training and collected results for the model: 5\n",
      "Completed training and collected results for the model: 6\n",
      "Completed training and collected results for the model: 7\n",
      "Completed training and collected results for the model: 8\n",
      "Completed training and collected results for the model: 9\n",
      "Completed training and collected results for the model: 10\n",
      "Completed training and collected results for the model: 11\n",
      "Completed training and collected results for the model: 12\n",
      "Completed training and collected results for the model: 13\n",
      "Completed training and collected results for the model: 14\n",
      "Completed training and collected results for the model: 15\n",
      "Completed training and collected results for the model: 16\n",
      "Completed training and collected results for the model: 17\n",
      "Completed training and collected results for the model: 18\n",
      "Completed training and collected results for the model: 19\n",
      "Completed training and collected results for the model: 20\n"
     ]
    }
   ],
   "source": [
    "model_architecture_list, learning_rate_list, cost_list, accuracy_list = \\\n",
    "caviar_strategy_search(model_num, batch_size, './iris_models/',randome_seed = randome_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store the results in pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"model layers\": model_architecture_list,\n",
    "                        \"learning rate\": learning_rate_list,\n",
    "                        \"accuracy\": accuracy_list,\n",
    "                        \"cost\": cost_list})\n",
    "results = results.reindex(columns=[\"model layers\", \"learning rate\", \"accuracy\", \"cost\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model layers</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4, 13, 8, 11, 3]</td>\n",
       "      <td>0.4389</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.066512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4, 11, 13, 10, 12, 3]</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.022759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[4, 16, 16, 9, 14, 15, 3]</td>\n",
       "      <td>0.3146</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.018037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4, 16, 9, 13, 16, 12, 3]</td>\n",
       "      <td>0.4842</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.029429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[4, 11, 13, 8, 3]</td>\n",
       "      <td>0.2015</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.035190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[4, 11, 16, 9, 11, 11, 3]</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.062448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[4, 15, 8, 9, 8, 12, 3]</td>\n",
       "      <td>0.2829</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.014656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[4, 10, 15, 10, 8, 8, 3]</td>\n",
       "      <td>0.0704</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.037708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[4, 13, 13, 14, 3]</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.026099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[4, 12, 9, 12, 3]</td>\n",
       "      <td>0.1314</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.020050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[4, 16, 9, 9, 15, 3]</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.042310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[4, 11, 14, 15, 10, 3]</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.033119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[4, 11, 13, 12, 3]</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.044441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[4, 14, 12, 12, 3]</td>\n",
       "      <td>0.3666</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.052081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[4, 12, 12, 16, 3]</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.042011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[4, 12, 11, 15, 13, 13, 3]</td>\n",
       "      <td>0.4864</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.029379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[4, 9, 13, 11, 3]</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.038698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[4, 13, 8, 9, 3]</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.019672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[4, 12, 10, 8, 11, 10, 3]</td>\n",
       "      <td>0.4435</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.040128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[4, 8, 15, 13, 8, 10, 3]</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.027789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model layers  learning rate  accuracy      cost\n",
       "0            [4, 13, 8, 11, 3]         0.4389     0.950  0.066512\n",
       "1       [4, 11, 13, 10, 12, 3]         0.1563     0.975  0.022759\n",
       "2    [4, 16, 16, 9, 14, 15, 3]         0.3146     0.975  0.018037\n",
       "3    [4, 16, 9, 13, 16, 12, 3]         0.4842     0.975  0.029429\n",
       "4            [4, 11, 13, 8, 3]         0.2015     0.950  0.035190\n",
       "5    [4, 11, 16, 9, 11, 11, 3]         0.4910     0.975  0.062448\n",
       "6      [4, 15, 8, 9, 8, 12, 3]         0.2829     0.925  0.014656\n",
       "7     [4, 10, 15, 10, 8, 8, 3]         0.0704     0.975  0.037708\n",
       "8           [4, 13, 13, 14, 3]         0.0975     0.975  0.026099\n",
       "9            [4, 12, 9, 12, 3]         0.1314     0.975  0.020050\n",
       "10        [4, 16, 9, 9, 15, 3]         0.2467     0.975  0.042310\n",
       "11      [4, 11, 14, 15, 10, 3]         0.2083     0.975  0.033119\n",
       "12          [4, 11, 13, 12, 3]         0.2490     0.975  0.044441\n",
       "13          [4, 14, 12, 12, 3]         0.3666     0.975  0.052081\n",
       "14          [4, 12, 12, 16, 3]         0.1812     0.950  0.042011\n",
       "15  [4, 12, 11, 15, 13, 13, 3]         0.4864     0.975  0.029379\n",
       "16           [4, 9, 13, 11, 3]         0.1382     0.950  0.038698\n",
       "17            [4, 13, 8, 9, 3]         0.2467     0.975  0.019672\n",
       "18   [4, 12, 10, 8, 11, 10, 3]         0.4435     0.975  0.040128\n",
       "19    [4, 8, 15, 13, 8, 10, 3]         0.2490     0.975  0.027789"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used caviar search strategy to tune just 2 parameters beaucse my neural network class is limited. However, this strategy can be used to tune other hyperparameters like activation function, dropout probability, regularization lamba parameter, and others. Engineers dont use grid search for hyperparameter tuning for deep learning models; the reason is it sometimes takes days and weeks to train a deep learning model, a grid search is just not feasible; hence random search through hypermeter space is the most effective strategy, this is what caviar strategy does. The caviar search strategy has been encouraged by deep learning scientists like Andrew Ng and Ian Goodfellow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
