{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS DATA CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains neural network to classify iris data. The notebook also shows the caviar search strategy for hyperparameter tuning, a technique is followed by machine learning scientists and engineers for training deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import deep_learn package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported deep_learn from enviroment site package\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from deep_learn.nn import ann\n",
    "    print(\"Imported deep_learn from enviroment site package\")\n",
    "except:\n",
    "    from config import *\n",
    "    append_path('../')\n",
    "    from deep_learn.nn import ann\n",
    "    print(\"Imported deep_learn by appending the path of the package to system path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import neccessary packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and reshape data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load iris data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store the data in pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the X y data horizontally\n",
    "data = np.hstack((X,y))\n",
    "# store the numpy array in pandas dataframe\n",
    "data = pd.DataFrame(data=data, columns=iris.feature_names +['species'])\n",
    "# shuffle the data\n",
    "data = data.sample(frac=1, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                 5.8               4.0                1.2               0.2   \n",
       "1                 5.1               2.5                3.0               1.1   \n",
       "2                 6.6               3.0                4.4               1.4   \n",
       "3                 5.4               3.9                1.3               0.4   \n",
       "4                 7.9               3.8                6.4               2.0   \n",
       "5                 6.3               3.3                4.7               1.6   \n",
       "6                 6.9               3.1                5.1               2.3   \n",
       "7                 5.1               3.8                1.9               0.4   \n",
       "8                 4.7               3.2                1.6               0.2   \n",
       "9                 6.9               3.2                5.7               2.3   \n",
       "10                5.6               2.7                4.2               1.3   \n",
       "11                5.4               3.9                1.7               0.4   \n",
       "12                7.1               3.0                5.9               2.1   \n",
       "13                6.4               3.2                4.5               1.5   \n",
       "14                6.0               2.9                4.5               1.5   \n",
       "15                4.4               3.2                1.3               0.2   \n",
       "16                5.8               2.6                4.0               1.2   \n",
       "17                5.6               3.0                4.5               1.5   \n",
       "18                5.4               3.4                1.5               0.4   \n",
       "19                5.0               3.2                1.2               0.2   \n",
       "\n",
       "    species  \n",
       "0       0.0  \n",
       "1       1.0  \n",
       "2       1.0  \n",
       "3       0.0  \n",
       "4       2.0  \n",
       "5       1.0  \n",
       "6       2.0  \n",
       "7       0.0  \n",
       "8       0.0  \n",
       "9       2.0  \n",
       "10      1.0  \n",
       "11      0.0  \n",
       "12      2.0  \n",
       "13      1.0  \n",
       "14      1.0  \n",
       "15      0.0  \n",
       "16      1.0  \n",
       "17      1.0  \n",
       "18      0.0  \n",
       "19      0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features and output of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = iris.feature_names\n",
    "output = 'species'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data for deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:` I did not use a separate validation set in this excercise since the iris data contains only 150 examples. It is difficult to split such a small data into train, validation, and test sets. According to literature it is ok to leave out the validation set in such case. For such a small data K fold cross validation is a viable option; but K fold cross validation is used for normal machine learning algorithms, not deep learning algorithms. Deep learning algorithms works best in the big data regime, training deep learning models takes more time than training normal machine learning models. Hence, engineers do not use K fold cross validation for training deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do a train test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size = 0.266, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A function to extract feature matrix and output vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy_data(dataframe, features = None, output = None):\n",
    "\n",
    "    '''a function for parsing the feature matrix and output array from a pandas dataframe'''\n",
    "\n",
    "    # to ignore pandas warning\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # import numpy\n",
    "    import numpy as np\n",
    "\n",
    "    # if no featues are given then just return the a numpy matrix of the dataframe\n",
    "    if features == None:\n",
    "        return dataframe.as_matrix()\n",
    "\n",
    "    # extract the feature matrix and convert it to numpy array\n",
    "    X = dataframe[features].as_matrix()\n",
    "\n",
    "    # if there is no output\n",
    "    if output == None:\n",
    "        return X\n",
    "    # if the output vector is wanted by the user\n",
    "    else:\n",
    "        # extracting the output columns and converting it to numpy array\n",
    "        y = dataframe[output].as_matrix()\n",
    "        y = np.reshape(y, (-1,1))\n",
    "        # returning the feature matrix and output vector\n",
    "        return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract X y data for train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_xy_data(train_data, features=features, output=output)\n",
    "X_test, Y_test = get_xy_data(test_data, features=features, output=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Onehot encoding the y data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "Y_train = Y_train.toarray()\n",
    "Y_test = encoder.transform(Y_test)\n",
    "Y_test = Y_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110, 4)\n",
      "(110, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 4)\n",
      "(40, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the first neural network for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of first neural network is a dirty implementation which allows machine learning engineers to test if the network along with its hyperparameters, architecture, and loss function actually works. After creating the dirty implementation engineers do hyperparameter tuning. Now the iris dataset is a very simple data to create a very accurate first implementation. Things will not be so easy for example creating a yolo object detection network for detecting vehicles, pedestrians, and road signs for self driving system.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural network architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [4,4,8,8,4,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a nn model object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ann(layers_dims=layers_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = X_train.shape[0]\n",
    "learning_rate = 0.1*.5\n",
    "num_iterations = 40000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss after iteration 0: 1.141538\n",
      "Log loss after iteration 100: 1.097231\n",
      "Log loss after iteration 200: 1.101953\n",
      "Log loss after iteration 300: 1.097959\n",
      "Log loss after iteration 400: 1.095068\n",
      "Log loss after iteration 500: 1.095910\n",
      "Log loss after iteration 600: 1.096005\n",
      "Log loss after iteration 700: 1.098199\n",
      "Log loss after iteration 800: 1.102550\n",
      "Log loss after iteration 900: 1.099853\n",
      "Log loss after iteration 1000: 1.095382\n",
      "Log loss after iteration 1100: 1.099318\n",
      "Log loss after iteration 1200: 1.088701\n",
      "Log loss after iteration 1300: 1.095299\n",
      "Log loss after iteration 1400: 1.096558\n",
      "Log loss after iteration 1500: 1.096727\n",
      "Log loss after iteration 1600: 1.098543\n",
      "Log loss after iteration 1700: 1.099408\n",
      "Log loss after iteration 1800: 1.101593\n",
      "Log loss after iteration 1900: 1.097817\n",
      "Log loss after iteration 2000: 1.098948\n",
      "Log loss after iteration 2100: 1.099111\n",
      "Log loss after iteration 2200: 1.097732\n",
      "Log loss after iteration 2300: 1.097172\n",
      "Log loss after iteration 2400: 1.094945\n",
      "Log loss after iteration 2500: 1.096410\n",
      "Log loss after iteration 2600: 1.095079\n",
      "Log loss after iteration 2700: 1.092685\n",
      "Log loss after iteration 2800: 1.093785\n",
      "Log loss after iteration 2900: 1.096380\n",
      "Log loss after iteration 3000: 1.085926\n",
      "Log loss after iteration 3100: 1.086663\n",
      "Log loss after iteration 3200: 1.086084\n",
      "Log loss after iteration 3300: 1.086688\n",
      "Log loss after iteration 3400: 1.079885\n",
      "Log loss after iteration 3500: 1.071297\n",
      "Log loss after iteration 3600: 1.065327\n",
      "Log loss after iteration 3700: 1.057986\n",
      "Log loss after iteration 3800: 1.013874\n",
      "Log loss after iteration 3900: 0.995266\n",
      "Log loss after iteration 4000: 0.924267\n",
      "Log loss after iteration 4100: 0.836937\n",
      "Log loss after iteration 4200: 0.784408\n",
      "Log loss after iteration 4300: 0.627154\n",
      "Log loss after iteration 4400: 0.585327\n",
      "Log loss after iteration 4500: 0.529606\n",
      "Log loss after iteration 4600: 0.517681\n",
      "Log loss after iteration 4700: 0.486039\n",
      "Log loss after iteration 4800: 0.548227\n",
      "Log loss after iteration 4900: 0.521728\n",
      "Log loss after iteration 5000: 0.451106\n",
      "Log loss after iteration 5100: 0.521100\n",
      "Log loss after iteration 5200: 0.466852\n",
      "Log loss after iteration 5300: 0.475298\n",
      "Log loss after iteration 5400: 0.484668\n",
      "Log loss after iteration 5500: 0.457975\n",
      "Log loss after iteration 5600: 0.468992\n",
      "Log loss after iteration 5700: 0.491565\n",
      "Log loss after iteration 5800: 0.457090\n",
      "Log loss after iteration 5900: 0.494358\n",
      "Log loss after iteration 6000: 0.540699\n",
      "Log loss after iteration 6100: 0.425706\n",
      "Log loss after iteration 6200: 0.473048\n",
      "Log loss after iteration 6300: 0.444992\n",
      "Log loss after iteration 6400: 0.450718\n",
      "Log loss after iteration 6500: 0.409078\n",
      "Log loss after iteration 6600: 0.507683\n",
      "Log loss after iteration 6700: 0.514407\n",
      "Log loss after iteration 6800: 0.475052\n",
      "Log loss after iteration 6900: 0.447033\n",
      "Log loss after iteration 7000: 0.427223\n",
      "Log loss after iteration 7100: 0.414007\n",
      "Log loss after iteration 7200: 0.449520\n",
      "Log loss after iteration 7300: 0.458441\n",
      "Log loss after iteration 7400: 0.410357\n",
      "Log loss after iteration 7500: 0.465695\n",
      "Log loss after iteration 7600: 0.403759\n",
      "Log loss after iteration 7700: 0.410157\n",
      "Log loss after iteration 7800: 0.432819\n",
      "Log loss after iteration 7900: 0.349529\n",
      "Log loss after iteration 8000: 0.372084\n",
      "Log loss after iteration 8100: 0.329165\n",
      "Log loss after iteration 8200: 0.302499\n",
      "Log loss after iteration 8300: 0.296917\n",
      "Log loss after iteration 8400: 0.284296\n",
      "Log loss after iteration 8500: 0.235990\n",
      "Log loss after iteration 8600: 0.217064\n",
      "Log loss after iteration 8700: 0.200890\n",
      "Log loss after iteration 8800: 0.233887\n",
      "Log loss after iteration 8900: 0.213202\n",
      "Log loss after iteration 9000: 0.175328\n",
      "Log loss after iteration 9100: 0.246614\n",
      "Log loss after iteration 9200: 0.148774\n",
      "Log loss after iteration 9300: 0.147712\n",
      "Log loss after iteration 9400: 0.143669\n",
      "Log loss after iteration 9500: 0.125833\n",
      "Log loss after iteration 9600: 0.136769\n",
      "Log loss after iteration 9700: 0.161923\n",
      "Log loss after iteration 9800: 0.120981\n",
      "Log loss after iteration 9900: 0.123116\n",
      "Log loss after iteration 10000: 0.144125\n",
      "Log loss after iteration 10100: 0.127515\n",
      "Log loss after iteration 10200: 0.092186\n",
      "Log loss after iteration 10300: 0.076727\n",
      "Log loss after iteration 10400: 0.107118\n",
      "Log loss after iteration 10500: 0.077702\n",
      "Log loss after iteration 10600: 0.071347\n",
      "Log loss after iteration 10700: 0.072004\n",
      "Log loss after iteration 10800: 0.076023\n",
      "Log loss after iteration 10900: 0.134649\n",
      "Log loss after iteration 11000: 0.077602\n",
      "Log loss after iteration 11100: 0.060490\n",
      "Log loss after iteration 11200: 0.089926\n",
      "Log loss after iteration 11300: 0.080683\n",
      "Log loss after iteration 11400: 0.093115\n",
      "Log loss after iteration 11500: 0.094916\n",
      "Log loss after iteration 11600: 0.063480\n",
      "Log loss after iteration 11700: 0.080398\n",
      "Log loss after iteration 11800: 0.049399\n",
      "Log loss after iteration 11900: 0.052444\n",
      "Log loss after iteration 12000: 0.068840\n",
      "Log loss after iteration 12100: 0.110633\n",
      "Log loss after iteration 12200: 0.117500\n",
      "Log loss after iteration 12300: 0.171030\n",
      "Log loss after iteration 12400: 0.104685\n",
      "Log loss after iteration 12500: 0.044344\n",
      "Log loss after iteration 12600: 0.076577\n",
      "Log loss after iteration 12700: 0.094206\n",
      "Log loss after iteration 12800: 0.084340\n",
      "Log loss after iteration 12900: 0.090057\n",
      "Log loss after iteration 13000: 0.066107\n",
      "Log loss after iteration 13100: 0.094670\n",
      "Log loss after iteration 13200: 0.094415\n",
      "Log loss after iteration 13300: 0.063620\n",
      "Log loss after iteration 13400: 0.051361\n",
      "Log loss after iteration 13500: 0.097158\n",
      "Log loss after iteration 13600: 0.069109\n",
      "Log loss after iteration 13700: 0.041060\n",
      "Log loss after iteration 13800: 0.085837\n",
      "Log loss after iteration 13900: 0.047142\n",
      "Log loss after iteration 14000: 0.086959\n",
      "Log loss after iteration 14100: 0.091103\n",
      "Log loss after iteration 14200: 0.075235\n",
      "Log loss after iteration 14300: 0.039425\n",
      "Log loss after iteration 14400: 0.041641\n",
      "Log loss after iteration 14500: 0.087002\n",
      "Log loss after iteration 14600: 0.056866\n",
      "Log loss after iteration 14700: 0.067571\n",
      "Log loss after iteration 14800: 0.040358\n",
      "Log loss after iteration 14900: 0.045372\n",
      "Log loss after iteration 15000: 0.065317\n",
      "Log loss after iteration 15100: 0.052113\n",
      "Log loss after iteration 15200: 0.062728\n",
      "Log loss after iteration 15300: 0.052232\n",
      "Log loss after iteration 15400: 0.045335\n",
      "Log loss after iteration 15500: 0.037355\n",
      "Log loss after iteration 15600: 0.059230\n",
      "Log loss after iteration 15700: 0.144452\n",
      "Log loss after iteration 15800: 0.068376\n",
      "Log loss after iteration 15900: 0.049710\n",
      "Log loss after iteration 16000: 0.057913\n",
      "Log loss after iteration 16100: 0.060318\n",
      "Log loss after iteration 16200: 0.050659\n",
      "Log loss after iteration 16300: 0.124472\n",
      "Log loss after iteration 16400: 0.057547\n",
      "Log loss after iteration 16500: 0.088775\n",
      "Log loss after iteration 16600: 0.036774\n",
      "Log loss after iteration 16700: 0.089323\n",
      "Log loss after iteration 16800: 0.033823\n",
      "Log loss after iteration 16900: 0.093966\n",
      "Log loss after iteration 17000: 0.183675\n",
      "Log loss after iteration 17100: 0.059407\n",
      "Log loss after iteration 17200: 0.046297\n",
      "Log loss after iteration 17300: 0.025516\n",
      "Log loss after iteration 17400: 0.029251\n",
      "Log loss after iteration 17500: 0.060717\n",
      "Log loss after iteration 17600: 0.091489\n",
      "Log loss after iteration 17700: 0.075355\n",
      "Log loss after iteration 17800: 0.031489\n",
      "Log loss after iteration 17900: 0.051996\n",
      "Log loss after iteration 18000: 0.167617\n",
      "Log loss after iteration 18100: 0.043271\n",
      "Log loss after iteration 18200: 0.048977\n",
      "Log loss after iteration 18300: 0.044606\n",
      "Log loss after iteration 18400: 0.057873\n",
      "Log loss after iteration 18500: 0.079249\n",
      "Log loss after iteration 18600: 0.031190\n",
      "Log loss after iteration 18700: 0.027122\n",
      "Log loss after iteration 18800: 0.070908\n",
      "Log loss after iteration 18900: 0.104234\n",
      "Log loss after iteration 19000: 0.016092\n",
      "Log loss after iteration 19100: 0.085568\n",
      "Log loss after iteration 19200: 0.026616\n",
      "Log loss after iteration 19300: 0.052972\n",
      "Log loss after iteration 19400: 0.018711\n",
      "Log loss after iteration 19500: 0.068132\n",
      "Log loss after iteration 19600: 0.019239\n",
      "Log loss after iteration 19700: 0.057989\n",
      "Log loss after iteration 19800: 0.027523\n",
      "Log loss after iteration 19900: 0.026035\n",
      "Log loss after iteration 20000: 0.083647\n",
      "Log loss after iteration 20100: 0.116517\n",
      "Log loss after iteration 20200: 0.050174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss after iteration 20300: 0.027252\n",
      "Log loss after iteration 20400: 0.056130\n",
      "Log loss after iteration 20500: 0.085318\n",
      "Log loss after iteration 20600: 0.017564\n",
      "Log loss after iteration 20700: 0.032119\n",
      "Log loss after iteration 20800: 0.050925\n",
      "Log loss after iteration 20900: 0.021125\n",
      "Log loss after iteration 21000: 0.034156\n",
      "Log loss after iteration 21100: 0.032036\n",
      "Log loss after iteration 21200: 0.107726\n",
      "Log loss after iteration 21300: 0.089142\n",
      "Log loss after iteration 21400: 0.018240\n",
      "Log loss after iteration 21500: 0.024275\n",
      "Log loss after iteration 21600: 0.020017\n",
      "Log loss after iteration 21700: 0.015403\n",
      "Log loss after iteration 21800: 0.105771\n",
      "Log loss after iteration 21900: 0.060250\n",
      "Log loss after iteration 22000: 0.100183\n",
      "Log loss after iteration 22100: 0.169862\n",
      "Log loss after iteration 22200: 0.052679\n",
      "Log loss after iteration 22300: 0.051355\n",
      "Log loss after iteration 22400: 0.046148\n",
      "Log loss after iteration 22500: 0.109101\n",
      "Log loss after iteration 22600: 0.027459\n",
      "Log loss after iteration 22700: 0.083282\n",
      "Log loss after iteration 22800: 0.050851\n",
      "Log loss after iteration 22900: 0.044547\n",
      "Log loss after iteration 23000: 0.057898\n",
      "Log loss after iteration 23100: 0.077795\n",
      "Log loss after iteration 23200: 0.013936\n",
      "Log loss after iteration 23300: 0.055414\n",
      "Log loss after iteration 23400: 0.124773\n",
      "Log loss after iteration 23500: 0.066637\n",
      "Log loss after iteration 23600: 0.056938\n",
      "Log loss after iteration 23700: 0.050754\n",
      "Log loss after iteration 23800: 0.018727\n",
      "Log loss after iteration 23900: 0.084728\n",
      "Log loss after iteration 24000: 0.052523\n",
      "Log loss after iteration 24100: 0.115389\n",
      "Log loss after iteration 24200: 0.116157\n",
      "Log loss after iteration 24300: 0.083357\n",
      "Log loss after iteration 24400: 0.016763\n",
      "Log loss after iteration 24500: 0.092948\n",
      "Log loss after iteration 24600: 0.044386\n",
      "Log loss after iteration 24700: 0.112470\n",
      "Log loss after iteration 24800: 0.137813\n",
      "Log loss after iteration 24900: 0.075653\n",
      "Log loss after iteration 25000: 0.051496\n",
      "Log loss after iteration 25100: 0.079519\n",
      "Log loss after iteration 25200: 0.021277\n",
      "Log loss after iteration 25300: 0.075983\n",
      "Log loss after iteration 25400: 0.083984\n",
      "Log loss after iteration 25500: 0.104605\n",
      "Log loss after iteration 25600: 0.019768\n",
      "Log loss after iteration 25700: 0.081126\n",
      "Log loss after iteration 25800: 0.017601\n",
      "Log loss after iteration 25900: 0.090737\n",
      "Log loss after iteration 26000: 0.017524\n",
      "Log loss after iteration 26100: 0.044486\n",
      "Log loss after iteration 26200: 0.043170\n",
      "Log loss after iteration 26300: 0.019579\n",
      "Log loss after iteration 26400: 0.011469\n",
      "Log loss after iteration 26500: 0.049445\n",
      "Log loss after iteration 26600: 0.053665\n",
      "Log loss after iteration 26700: 0.011038\n",
      "Log loss after iteration 26800: 0.060985\n",
      "Log loss after iteration 26900: 0.048634\n",
      "Log loss after iteration 27000: 0.048783\n",
      "Log loss after iteration 27100: 0.049509\n",
      "Log loss after iteration 27200: 0.017901\n",
      "Log loss after iteration 27300: 0.047500\n",
      "Log loss after iteration 27400: 0.034137\n",
      "Log loss after iteration 27500: 0.107944\n",
      "Log loss after iteration 27600: 0.046724\n",
      "Log loss after iteration 27700: 0.083411\n",
      "Log loss after iteration 27800: 0.019248\n",
      "Log loss after iteration 27900: 0.051373\n",
      "Log loss after iteration 28000: 0.051630\n",
      "Log loss after iteration 28100: 0.154676\n",
      "Log loss after iteration 28200: 0.014475\n",
      "Log loss after iteration 28300: 0.011534\n",
      "Log loss after iteration 28400: 0.079046\n",
      "Log loss after iteration 28500: 0.024172\n",
      "Log loss after iteration 28600: 0.089739\n",
      "Log loss after iteration 28700: 0.050234\n",
      "Log loss after iteration 28800: 0.017284\n",
      "Log loss after iteration 28900: 0.020675\n",
      "Log loss after iteration 29000: 0.083822\n",
      "Log loss after iteration 29100: 0.015396\n",
      "Log loss after iteration 29200: 0.014831\n",
      "Log loss after iteration 29300: 0.019092\n",
      "Log loss after iteration 29400: 0.013592\n",
      "Log loss after iteration 29500: 0.015956\n",
      "Log loss after iteration 29600: 0.011959\n",
      "Log loss after iteration 29700: 0.048825\n",
      "Log loss after iteration 29800: 0.079506\n",
      "Log loss after iteration 29900: 0.078767\n",
      "Log loss after iteration 30000: 0.018929\n",
      "Log loss after iteration 30100: 0.046713\n",
      "Log loss after iteration 30200: 0.049740\n",
      "Log loss after iteration 30300: 0.016665\n",
      "Log loss after iteration 30400: 0.015962\n",
      "Log loss after iteration 30500: 0.079708\n",
      "Log loss after iteration 30600: 0.011605\n",
      "Log loss after iteration 30700: 0.045254\n",
      "Log loss after iteration 30800: 0.043062\n",
      "Log loss after iteration 30900: 0.075973\n",
      "Log loss after iteration 31000: 0.083404\n",
      "Log loss after iteration 31100: 0.046072\n",
      "Log loss after iteration 31200: 0.075988\n",
      "Log loss after iteration 31300: 0.081139\n",
      "Log loss after iteration 31400: 0.047834\n",
      "Log loss after iteration 31500: 0.056627\n",
      "Log loss after iteration 31600: 0.046652\n",
      "Log loss after iteration 31700: 0.011921\n",
      "Log loss after iteration 31800: 0.042386\n",
      "Log loss after iteration 31900: 0.017118\n",
      "Log loss after iteration 32000: 0.044602\n",
      "Log loss after iteration 32100: 0.049470\n",
      "Log loss after iteration 32200: 0.074906\n",
      "Log loss after iteration 32300: 0.012929\n",
      "Log loss after iteration 32400: 0.146536\n",
      "Log loss after iteration 32500: 0.078071\n",
      "Log loss after iteration 32600: 0.048422\n",
      "Log loss after iteration 32700: 0.076857\n",
      "Log loss after iteration 32800: 0.061080\n",
      "Log loss after iteration 32900: 0.078548\n",
      "Log loss after iteration 33000: 0.046067\n",
      "Log loss after iteration 33100: 0.109253\n",
      "Log loss after iteration 33200: 0.052368\n",
      "Log loss after iteration 33300: 0.109296\n",
      "Log loss after iteration 33400: 0.015114\n",
      "Log loss after iteration 33500: 0.046102\n",
      "Log loss after iteration 33600: 0.010742\n",
      "Log loss after iteration 33700: 0.010537\n",
      "Log loss after iteration 33800: 0.047841\n",
      "Log loss after iteration 33900: 0.080777\n",
      "Log loss after iteration 34000: 0.014627\n",
      "Log loss after iteration 34100: 0.110278\n",
      "Log loss after iteration 34200: 0.046011\n",
      "Log loss after iteration 34300: 0.078266\n",
      "Log loss after iteration 34400: 0.108602\n",
      "Log loss after iteration 34500: 0.107082\n",
      "Log loss after iteration 34600: 0.043325\n",
      "Log loss after iteration 34700: 0.009922\n",
      "Log loss after iteration 34800: 0.113525\n",
      "Log loss after iteration 34900: 0.010147\n",
      "Log loss after iteration 35000: 0.009856\n",
      "Log loss after iteration 35100: 0.052732\n",
      "Log loss after iteration 35200: 0.014798\n",
      "Log loss after iteration 35300: 0.110366\n",
      "Log loss after iteration 35400: 0.044366\n",
      "Log loss after iteration 35500: 0.011175\n",
      "Log loss after iteration 35600: 0.042309\n",
      "Log loss after iteration 35700: 0.012535\n",
      "Log loss after iteration 35800: 0.010831\n",
      "Log loss after iteration 35900: 0.014372\n",
      "Log loss after iteration 36000: 0.047036\n",
      "Log loss after iteration 36100: 0.043836\n",
      "Log loss after iteration 36200: 0.056075\n",
      "Log loss after iteration 36300: 0.045658\n",
      "Log loss after iteration 36400: 0.045166\n",
      "Log loss after iteration 36500: 0.075616\n",
      "Log loss after iteration 36600: 0.011896\n",
      "Log loss after iteration 36700: 0.042145\n",
      "Log loss after iteration 36800: 0.041663\n",
      "Log loss after iteration 36900: 0.012112\n",
      "Log loss after iteration 37000: 0.109495\n",
      "Log loss after iteration 37100: 0.044709\n",
      "Log loss after iteration 37200: 0.014499\n",
      "Log loss after iteration 37300: 0.010141\n",
      "Log loss after iteration 37400: 0.050193\n",
      "Log loss after iteration 37500: 0.081021\n",
      "Log loss after iteration 37600: 0.044517\n",
      "Log loss after iteration 37700: 0.072723\n",
      "Log loss after iteration 37800: 0.010954\n",
      "Log loss after iteration 37900: 0.021951\n",
      "Log loss after iteration 38000: 0.046208\n",
      "Log loss after iteration 38100: 0.042099\n",
      "Log loss after iteration 38200: 0.078032\n",
      "Log loss after iteration 38300: 0.013275\n",
      "Log loss after iteration 38400: 0.047146\n",
      "Log loss after iteration 38500: 0.012360\n",
      "Log loss after iteration 38600: 0.044210\n",
      "Log loss after iteration 38700: 0.044475\n",
      "Log loss after iteration 38800: 0.044482\n",
      "Log loss after iteration 38900: 0.115442\n",
      "Log loss after iteration 39000: 0.010687\n",
      "Log loss after iteration 39100: 0.077500\n",
      "Log loss after iteration 39200: 0.016892\n",
      "Log loss after iteration 39300: 0.047485\n",
      "Log loss after iteration 39400: 0.011253\n",
      "Log loss after iteration 39500: 0.016864\n",
      "Log loss after iteration 39600: 0.136349\n",
      "Log loss after iteration 39700: 0.141368\n",
      "Log loss after iteration 39800: 0.045631\n",
      "Log loss after iteration 39900: 0.009295\n",
      "Log loss after iteration 40000: 0.108390\n",
      "Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, X_test, Y_test, batch_size,\n",
    "          learning_rate = learning_rate, \n",
    "          num_iterations = num_iterations, print_cost=True, random_seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot of Cost vs Iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.squeeze(model.costs))\n",
    "plt.ylabel('Log loss')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Plot of Cost vs Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Caviar Strategy for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 strategies for deep learning hyperparamter tuning: 1) Panda strategy in which we babysit a single model, this is applicable if computing resource is limited 2) Caviar strategy in which we randomly initialize a number of hyperparameter settings and train neural network models using the different settings and then choose the one with the lowest error, this is strategy is applicable if we have enormous computing resource. In this notebook I will choose to tune the parameters of learning rate and nn architecture (# hidden layers, # hidden layer nodes) using caviar strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to randomly generate a given number of nn architectures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer_and_node_generator(model_num, num_input, num_output, randome_seed = 0, low = 8, high=17):\n",
    "    \n",
    "    '''a function to randomly generate a given number of nn architectures'''\n",
    "    \n",
    "    # set the random seed\n",
    "    np.random.seed(randome_seed)\n",
    "    \n",
    "    # list to store the architectures\n",
    "    model_architecture_list = []\n",
    "    \n",
    "    # iterate given number of times\n",
    "    for i in range(model_num):\n",
    "        # randomly generate number of hidden layers\n",
    "        num_hidden = np.random.randint(low = 3, high = 6)\n",
    "        # randomly generate the number of nodes in each layer\n",
    "        layers_dims = np.random.randint(low = low, high = high, size = num_hidden)\n",
    "        layers_dims = layers_dims.tolist()\n",
    "        # insert the input and output layer\n",
    "        layers_dims.insert(0,num_input)\n",
    "        layers_dims.append(num_output)\n",
    "        # append the architecture to the designated list\n",
    "        model_architecture_list.append(layers_dims)\n",
    "    \n",
    "    return model_architecture_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a list of learning rates to choose from**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05  , 0.0523, 0.0545, 0.0568, 0.059 , 0.0613, 0.0636, 0.0658,\n",
       "       0.0681, 0.0704, 0.0726, 0.0749, 0.0771, 0.0794, 0.0817, 0.0839,\n",
       "       0.0862, 0.0884, 0.0907, 0.093 , 0.0952, 0.0975, 0.0997, 0.102 ,\n",
       "       0.1043, 0.1065, 0.1088, 0.1111, 0.1133, 0.1156, 0.1178, 0.1201,\n",
       "       0.1224, 0.1246, 0.1269, 0.1291, 0.1314, 0.1337, 0.1359, 0.1382,\n",
       "       0.1405, 0.1427, 0.145 , 0.1472, 0.1495, 0.1518, 0.154 , 0.1563,\n",
       "       0.1585, 0.1608, 0.1631, 0.1653, 0.1676, 0.1698, 0.1721, 0.1744,\n",
       "       0.1766, 0.1789, 0.1812, 0.1834, 0.1857, 0.1879, 0.1902, 0.1925,\n",
       "       0.1947, 0.197 , 0.1992, 0.2015, 0.2038, 0.206 , 0.2083, 0.2106,\n",
       "       0.2128, 0.2151, 0.2173, 0.2196, 0.2219, 0.2241, 0.2264, 0.2286,\n",
       "       0.2309, 0.2332, 0.2354, 0.2377, 0.2399, 0.2422, 0.2445, 0.2467,\n",
       "       0.249 , 0.2513, 0.2535, 0.2558, 0.258 , 0.2603, 0.2626, 0.2648,\n",
       "       0.2671, 0.2693, 0.2716, 0.2739, 0.2761, 0.2784, 0.2807, 0.2829,\n",
       "       0.2852, 0.2874, 0.2897, 0.292 , 0.2942, 0.2965, 0.2987, 0.301 ,\n",
       "       0.3033, 0.3055, 0.3078, 0.3101, 0.3123, 0.3146, 0.3168, 0.3191,\n",
       "       0.3214, 0.3236, 0.3259, 0.3281, 0.3304, 0.3327, 0.3349, 0.3372,\n",
       "       0.3394, 0.3417, 0.344 , 0.3462, 0.3485, 0.3508, 0.353 , 0.3553,\n",
       "       0.3575, 0.3598, 0.3621, 0.3643, 0.3666, 0.3688, 0.3711, 0.3734,\n",
       "       0.3756, 0.3779, 0.3802, 0.3824, 0.3847, 0.3869, 0.3892, 0.3915,\n",
       "       0.3937, 0.396 , 0.3982, 0.4005, 0.4028, 0.405 , 0.4073, 0.4095,\n",
       "       0.4118, 0.4141, 0.4163, 0.4186, 0.4209, 0.4231, 0.4254, 0.4276,\n",
       "       0.4299, 0.4322, 0.4344, 0.4367, 0.4389, 0.4412, 0.4435, 0.4457,\n",
       "       0.448 , 0.4503, 0.4525, 0.4548, 0.457 , 0.4593, 0.4616, 0.4638,\n",
       "       0.4661, 0.4683, 0.4706, 0.4729, 0.4751, 0.4774, 0.4796, 0.4819,\n",
       "       0.4842, 0.4864, 0.4887, 0.491 , 0.4932, 0.4955, 0.4977, 0.5   ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates = np.round(np.linspace(0.1*0.5,0.1*5,num=200),4)\n",
    "learning_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to randomly generate a given number of learning rates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_generator(learning_rates, model_num, randome_seed = 0):\n",
    "    \n",
    "    '''a function to randomly generate a given number of learning rates'''\n",
    "    \n",
    "    np.random.seed(randome_seed)\n",
    "    \n",
    "    return np.random.choice(learning_rates, size=model_num).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function which implements a caviar strategy search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caviar_strategy_search(model_num, batch_size, model_directory, randome_seed = 0, num_iterations = 40000):\n",
    "    \n",
    "    '''a function which implements a caviar strategy search'''\n",
    "    \n",
    "    # randomly generate a list of learning rates\n",
    "    learning_rate_list = learning_rate_generator(learning_rates, model_num, randome_seed = randome_seed)\n",
    "    # randomly generate a list of architectures \n",
    "    model_architecture_list = hidden_layer_and_node_generator(model_num,4,3, randome_seed = randome_seed)\n",
    "    # lists to store the costs and accuracy of models\n",
    "    cost_list = []\n",
    "    accuracy_list = []\n",
    "    \n",
    "    # iterate a given number of times\n",
    "    for i in range(model_num):\n",
    "        # create and fit a nn model with given architecture\n",
    "        model = ann(layers_dims=model_architecture_list[i])\n",
    "        model.fit(X_train, Y_train, X_test, Y_test, batch_size,\n",
    "                  learning_rate = learning_rate_list[i], \n",
    "                  num_iterations = num_iterations, print_cost=False, random_seed = randome_seed)\n",
    "        \n",
    "        #pickling the model\n",
    "        pickle_out = open(model_directory+\"iris_model_\"+str(i+1)+\".mdl\",\"wb\")\n",
    "        pickle.dump(model, pickle_out)\n",
    "        pickle_out.close()\n",
    "        \n",
    "        # append the average of last 4 costs to the designated list\n",
    "        cost_list.append(np.average(model.costs[-4:]))\n",
    "        # append the accuracy to the designated list\n",
    "        accuracy_list.append(model.accuracy)\n",
    "        \n",
    "        # print statement\n",
    "        print(\"Completed training and collected results for the model:\",str(i+1))\n",
    "        \n",
    "    return model_architecture_list, learning_rate_list, cost_list, accuracy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do a caviar strategy search of hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of models\n",
    "model_num = 20\n",
    "# batch size\n",
    "batch_size = X_train.shape[0]\n",
    "# random seed\n",
    "randome_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed training and collected results for the model: 1\n",
      "Completed training and collected results for the model: 2\n",
      "Completed training and collected results for the model: 3\n",
      "Completed training and collected results for the model: 4\n",
      "Completed training and collected results for the model: 5\n",
      "Completed training and collected results for the model: 6\n",
      "Completed training and collected results for the model: 7\n",
      "Completed training and collected results for the model: 8\n",
      "Completed training and collected results for the model: 9\n",
      "Completed training and collected results for the model: 10\n",
      "Completed training and collected results for the model: 11\n",
      "Completed training and collected results for the model: 12\n",
      "Completed training and collected results for the model: 13\n",
      "Completed training and collected results for the model: 14\n",
      "Completed training and collected results for the model: 15\n",
      "Completed training and collected results for the model: 16\n",
      "Completed training and collected results for the model: 17\n",
      "Completed training and collected results for the model: 18\n",
      "Completed training and collected results for the model: 19\n",
      "Completed training and collected results for the model: 20\n"
     ]
    }
   ],
   "source": [
    "model_architecture_list, learning_rate_list, cost_list, accuracy_list = \\\n",
    "caviar_strategy_search(model_num, batch_size, './iris_models/',randome_seed = randome_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store the results in pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"model layers\": model_architecture_list,\n",
    "                        \"learning rate\": learning_rate_list,\n",
    "                        \"accuracy\": accuracy_list,\n",
    "                        \"Log loss\": cost_list})\n",
    "results = results.reindex(columns=[\"model layers\", \"learning rate\", \"accuracy\", \"Log loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model layers</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4, 13, 8, 11, 3]</td>\n",
       "      <td>0.4389</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.075639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4, 11, 13, 10, 12, 3]</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.026402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[4, 16, 16, 9, 14, 15, 3]</td>\n",
       "      <td>0.3146</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.017473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4, 16, 9, 13, 16, 12, 3]</td>\n",
       "      <td>0.4842</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.025969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[4, 11, 13, 8, 3]</td>\n",
       "      <td>0.2015</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.032692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[4, 11, 16, 9, 11, 11, 3]</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.075492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[4, 15, 8, 9, 8, 12, 3]</td>\n",
       "      <td>0.2829</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.017143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[4, 10, 15, 10, 8, 8, 3]</td>\n",
       "      <td>0.0704</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.038651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[4, 13, 13, 14, 3]</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.022086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[4, 12, 9, 12, 3]</td>\n",
       "      <td>0.1314</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.018482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[4, 16, 9, 9, 15, 3]</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.048033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[4, 11, 14, 15, 10, 3]</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.035065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[4, 11, 13, 12, 3]</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.050652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[4, 14, 12, 12, 3]</td>\n",
       "      <td>0.3666</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.055091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[4, 12, 12, 16, 3]</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.036852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[4, 12, 11, 15, 13, 13, 3]</td>\n",
       "      <td>0.4864</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.025989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[4, 9, 13, 11, 3]</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.034539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[4, 13, 8, 9, 3]</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.018135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[4, 12, 10, 8, 11, 10, 3]</td>\n",
       "      <td>0.4435</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.040274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[4, 8, 15, 13, 8, 10, 3]</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.022833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model layers  learning rate  accuracy  Log loss\n",
       "0            [4, 13, 8, 11, 3]         0.4389     0.950  0.075639\n",
       "1       [4, 11, 13, 10, 12, 3]         0.1563     0.975  0.026402\n",
       "2    [4, 16, 16, 9, 14, 15, 3]         0.3146     0.975  0.017473\n",
       "3    [4, 16, 9, 13, 16, 12, 3]         0.4842     0.975  0.025969\n",
       "4            [4, 11, 13, 8, 3]         0.2015     0.950  0.032692\n",
       "5    [4, 11, 16, 9, 11, 11, 3]         0.4910     0.975  0.075492\n",
       "6      [4, 15, 8, 9, 8, 12, 3]         0.2829     0.925  0.017143\n",
       "7     [4, 10, 15, 10, 8, 8, 3]         0.0704     0.975  0.038651\n",
       "8           [4, 13, 13, 14, 3]         0.0975     0.975  0.022086\n",
       "9            [4, 12, 9, 12, 3]         0.1314     0.975  0.018482\n",
       "10        [4, 16, 9, 9, 15, 3]         0.2467     0.975  0.048033\n",
       "11      [4, 11, 14, 15, 10, 3]         0.2083     0.975  0.035065\n",
       "12          [4, 11, 13, 12, 3]         0.2490     0.975  0.050652\n",
       "13          [4, 14, 12, 12, 3]         0.3666     0.975  0.055091\n",
       "14          [4, 12, 12, 16, 3]         0.1812     0.950  0.036852\n",
       "15  [4, 12, 11, 15, 13, 13, 3]         0.4864     0.975  0.025989\n",
       "16           [4, 9, 13, 11, 3]         0.1382     0.950  0.034539\n",
       "17            [4, 13, 8, 9, 3]         0.2467     0.975  0.018135\n",
       "18   [4, 12, 10, 8, 11, 10, 3]         0.4435     0.975  0.040274\n",
       "19    [4, 8, 15, 13, 8, 10, 3]         0.2490     0.975  0.022833"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like many models are tied with accuracy. In such a case model with lowest cost should be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used caviar strategy search to tune just 2 parameters beaucse my neural network class is limited. However, this strategy can be used to tune other hyperparameters like activation function, dropout probability, regularization lamba parameter, and others. Engineers dont use grid search for hyperparameter tuning of deep learning models; the reason is it sometimes takes days and weeks to train a deep learning model, a grid search is just not feasible. Hence random search through hypermeter space is the most effective strategy, this is what caviar strategy does. The caviar strategy search has been encouraged by deep learning scientists like Prof Andrew Ng and Ian Goodfellow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
